{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da6f45f6-3a1d-4659-9ae1-9c08295a7754",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('max_colwidth', 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "001eacf1-26d4-4318-90cb-9d6c8fc1e5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_verb_lemma_morpho(word, query_verb_dict_fnc, prefixes_list, use_longest_subword=True):\n",
    "\n",
    "    def get_verb_lemma_recursive(word, morphemes=[]):\n",
    "        '''\n",
    "        Recursively detect prefixes and yield all possible lemmas\n",
    "        '''\n",
    "        res = []\n",
    "        \n",
    "        base_lemma = query_verb_dict_fnc(word)\n",
    "        \n",
    "        if base_lemma:\n",
    "            #print(word,morphemes,base_lemma)\n",
    "            if len(morphemes)>1 and morphemes[-1] == 'zu' and base_lemma == word:\n",
    "                #suspect a zu-infinitive: some prefix on the right + zu + infinitive\n",
    "                #e.g. abzuheben, aufzuatmen\n",
    "                res.append((''.join(morphemes[:-1]+[base_lemma]),word)) #add without \"zu\"\n",
    "            else:\n",
    "                res.append((''.join(morphemes+[base_lemma]),word)) #compose the infinitive out of the collected prefixes and the base lemma\n",
    "                \n",
    "        for prefix_end_idx in range(1,len(word)-2):\n",
    "            #look for the next prefix which ends at prefix_end_idx\n",
    "            if word[:prefix_end_idx] in prefixes_list:\n",
    "                #prefix in the list of known prefixes\n",
    "                prefix = word[:prefix_end_idx] \n",
    "                res.extend(get_verb_lemma_recursive(word[prefix_end_idx:], morphemes+[prefix])) #detach the prefix, call the function again\n",
    "    \n",
    "        return res\n",
    "        \n",
    "    lemmas = get_verb_lemma_recursive(word)\n",
    "\n",
    "    if lemmas:\n",
    "        lemmas.sort(key=lambda x:-len(x[1])) #sort according to subword length, ascending=False\n",
    "        lemmas, subwords = zip(*lemmas)     \n",
    "        if (use_longest_subword or len(set(lemmas))==1):\n",
    "        #all possible splits lead to the same lemma or taking the longest subword allowed\n",
    "            return lemmas[0]\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23351203-fff8-4e1f-9257-73426b6719e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_noun_lemma_fwdsearch(word, query_noun_dict_fnc, use_longest_subword=True):\n",
    "\n",
    "    '''\n",
    "    Get all possible noun lemmas using forward search\n",
    "    '''\n",
    "\n",
    "    lemmas = []\n",
    "    \n",
    "    for start_idx in range(0,len(word)-2):\n",
    "        #remove letters one by one, until the rest of the word matches one in the vocabulary\n",
    "        trial_word = word[start_idx:]\n",
    "        base_lemma = query_noun_dict_fnc(trial_word)\n",
    "        if base_lemma:\n",
    "            #print(word[0:start_idx] + base_lemma, base_lemma)\n",
    "            lemmas.append(word[0:start_idx] + base_lemma)\n",
    "\n",
    "    if lemmas and (use_longest_subword or len(set(lemmas))==1):\n",
    "        #all possible splits lead to the same lemma or taking the longest subword allowed\n",
    "        return lemmas[0]\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8cd3461f-1e3b-4cbd-84c3-9f214db22a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_verb_lemma_fwdsearch(word, query_verb_dict_fnc, use_longest_subword=True):\n",
    "\n",
    "    '''\n",
    "    Get all possible verb lemmas using forward search, prefix-agnostic\n",
    "    '''\n",
    "\n",
    "    lemmas = []\n",
    "    \n",
    "    for start_idx in range(0,len(word)-2):\n",
    "        #remove letters one by one, until the rest of the word matches one in the vocabulary\n",
    "        trial_word = word[start_idx:]\n",
    "        base_lemma = query_verb_dict_fnc(trial_word)\n",
    "        if start_idx>2 and word[start_idx-2:start_idx] == 'zu' and base_lemma == trial_word:\n",
    "            #suspect a zu-infinitive: some prefix on the right + zu + infinitive\n",
    "            #e.g. abzuheben, aufzuatmen\n",
    "            lemmas.append(word[0:start_idx-2] + base_lemma) #add without \"zu\"\n",
    "        elif base_lemma and not base_lemma.startswith('zu'):\n",
    "            lemmas.append(word[0:start_idx] + base_lemma)\n",
    "\n",
    "    if lemmas and (use_longest_subword or len(set(lemmas))==1):\n",
    "        #all possible splits lead to the same lemma or taking the longest subword allowed\n",
    "        return lemmas[0]\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e397222c-2774-413e-a6db-67d67cd7f492",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_constraints = {'dieser': (('f', 'Genitiv Singular'), ('n', 'Genitiv Plural'), ('m', 'Genitiv Plural'), ('m', 'Nominativ Singular'), ('f', 'Dativ Singular'), ('f', 'Genitiv Plural'), ('only_plural', 'Genitiv Plural')), \n",
    " 'der': (('f', 'Genitiv Singular'), ('n', 'Genitiv Plural'), ('m', 'Genitiv Plural'), ('m', 'Nominativ Singular'), ('f', 'Dativ Singular'), ('f', 'Genitiv Plural'), ('only_plural', 'Genitiv Plural')), \n",
    " 'kein': (('m', 'Nominativ Singular'), ('n', 'Nominativ Singular'), ('n', 'Akkusativ Singular')), \n",
    " 'dieses': (('n', 'Nominativ Singular'), ('n', 'Akkusativ Singular'), ('m', 'Genitiv Singular'), ('n', 'Genitiv Singular')), \n",
    " 'des': (('m', 'Genitiv Singular'), ('n', 'Genitiv Singular')), 'keines': (('m', 'Genitiv Singular'), ('n', 'Genitiv Singular')), \n",
    " 'diesem': (('n', 'Dativ Singular'), ('m', 'Dativ Singular')), 'dem': (('n', 'Dativ Singular'), ('m', 'Dativ Singular')), \n",
    " 'keinem': (('n', 'Dativ Singular'), ('m', 'Dativ Singular')), 'diesen': (('n', 'Dativ Plural'), ('only_plural', 'Dativ Plural'), ('f', 'Dativ Plural'), ('m', 'Akkusativ Singular'), ('m', 'Dativ Plural')), \n",
    " 'den': (('n', 'Dativ Plural'), ('only_plural', 'Dativ Plural'), ('f', 'Dativ Plural'), ('m', 'Akkusativ Singular'), ('m', 'Dativ Plural')), \n",
    " 'keinen': (('n', 'Dativ Plural'), ('only_plural', 'Dativ Plural'), ('f', 'Dativ Plural'), ('m', 'Akkusativ Singular'), ('m', 'Dativ Plural')), \n",
    " 'die': (('only_plural', 'Nominativ Plural'), ('n', 'Akkusativ Plural'), ('f', 'Akkusativ Singular'), ('only_plural', 'Akkusativ Plural'), ('f', 'Nominativ Singular'), ('m', 'Nominativ Plural'), ('m', 'Akkusativ Plural'), ('f', 'Nominativ Plural'), ('n', 'Nominativ Plural'), ('f', 'Akkusativ Plural')), \n",
    " 'diese': (('only_plural', 'Nominativ Plural'), ('n', 'Akkusativ Plural'), ('f', 'Akkusativ Singular'), ('only_plural', 'Akkusativ Plural'), ('f', 'Nominativ Singular'), ('m', 'Nominativ Plural'), ('m', 'Akkusativ Plural'), ('f', 'Nominativ Plural'), ('n', 'Nominativ Plural'), ('f', 'Akkusativ Plural')), \n",
    " 'keine': (('only_plural', 'Nominativ Plural'), ('n', 'Akkusativ Plural'), ('f', 'Akkusativ Singular'), ('only_plural', 'Akkusativ Plural'), ('f', 'Nominativ Singular'), ('m', 'Nominativ Plural'), ('m', 'Akkusativ Plural'), ('f', 'Nominativ Plural'), ('n', 'Nominativ Plural'), ('f', 'Akkusativ Plural')), \n",
    " 'keiner': (('f', 'Genitiv Singular'), ('f', 'Genitiv Plural'), ('m', 'Genitiv Plural'), ('f', 'Dativ Singular'), ('n', 'Genitiv Plural'), ('only_plural', 'Genitiv Plural')), 'das': (('n', 'Nominativ Singular'), ('n', 'Akkusativ Singular'))\n",
    "}\n",
    "\n",
    "prep_constraints = {\n",
    "                 'zu': (('m','Dativ Singular'),('n','Dativ Singular'),('f','Dativ Singular'),('m','Dativ Plural'),('n','Dativ Plural'),('f','Dativ Plural')),\n",
    "                 'von': (('m','Dativ Singular'),('n','Dativ Singular'),('f','Dativ Singular'),('m','Dativ Plural'),('n','Dativ Plural'),('f','Dativ Plural')),\n",
    "                 'bei': (('m','Dativ Singular'),('n','Dativ Singular'),('f','Dativ Singular'),('m','Dativ Plural'),('n','Dativ Plural'),('f','Dativ Plural')),\n",
    "                 'durch': (('m','Akkusativ Singular'),('n','Akkusativ Singular'),('f','Akkusativ Singular'),('m','Akkusativ Plural'),('n','Akkusativ Plural'),('f','Akkusativ Plural')),\n",
    "                 'für': (('m','Akkusativ Singular'),('n','Akkusativ Singular'),('f','Akkusativ Singular'),('m','Akkusativ Plural'),('n','Akkusativ Plural'),('f','Akkusativ Plural')),\n",
    "                 'um': (('m','Akkusativ Singular'),('n','Akkusativ Singular'),('f','Akkusativ Singular'),('m','Akkusativ Plural'),('n','Akkusativ Plural'),('f','Akkusativ Plural')),\n",
    "                 'im':(('m','Dativ Singular'),('n','Dativ Singular')),\n",
    "                 'beim':(('m','Dativ Singular'),('n','Dativ Singular')),\n",
    "                 'zum':(('m','Dativ Singular'),('n','Dativ Singular')),\n",
    "                 'vom':(('m','Dativ Singular'),('n','Dativ Singular')),\n",
    "                 'zur':(('f','Dativ Singular'),),\n",
    "                 'hintern':(('m','Akkusativ Singular'),),\n",
    "                 'übern':(('m','Akkusativ Singular'),),\n",
    "                 'untern':(('m','Akkusativ Singular'),),\n",
    "                 'ins':(('n','Akkusativ Singular'),),\n",
    "                 'aufs':(('n','Akkusativ Singular'),),\n",
    "                 'durchs':(('n','Akkusativ Singular'),),\n",
    "                 'fürs':(('n','Akkusativ Singular'),),\n",
    "                 'ums':(('n','Akkusativ Singular'),),\n",
    "                 'vors':(('n','Akkusativ Singular'),),\n",
    "                 'übers':(('n','Akkusativ Singular'),),\n",
    "                 'unters':(('n','Akkusativ Singular'),),\n",
    "                 'hinterm':(('m','Dativ Singular'),('n','Dativ Singular')),\n",
    "                 'überm':(('m','Dativ Singular'),('n','Dativ Singular')),\n",
    "                 'unterm':(('m','Dativ Singular'),('n','Dativ Singular')),\n",
    "                 'vorm':(('m','Dativ Singular'),('n','Dativ Singular')),\n",
    "                }\n",
    "\n",
    "def get_base_determiner(word):\n",
    "    '''\n",
    "    Check if a given word is a determiner and return the base form\n",
    "    '''\n",
    "    kein_style = re.match(r'(mein|dein|sein|ihr|Ihr|euer|unser|ein|kein|welch|solch|manch)($|e[rsnm]?$)',word)\n",
    "    if kein_style:\n",
    "        return 'kein'+kein_style.groups()[1]\n",
    "    der_form = re.match(r'(der|die|das|dem|den|des)$',word)\n",
    "    if der_form:\n",
    "        return word\n",
    "    der_style = re.match(r'(diese|jede|jene)([rsnm]?$)',word)\n",
    "    if der_style:\n",
    "        return 'diese'+der_style.groups()[1]\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ddb936ce-de41-4d83-96b5-698aedef2fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NounsNBC():\n",
    "\n",
    "    def __init__(self, path):\n",
    "\n",
    "        with open(path,'rb') as f:\n",
    "\n",
    "            data = pickle.load(f)\n",
    "            \n",
    "            self.nbc_clf = data['clf']\n",
    "            self.features_encoder = data['features_encoder']\n",
    "            self.features_list = data['features_list']\n",
    "            self.rules_list = data['rules_list']\n",
    "            self.n_last = data['n_last']\n",
    "        \n",
    "    def __call__(self, word, constraints=None):\n",
    "        \n",
    "        word_parts = [word[idx:] for idx in range(-self.n_last,0)]\n",
    "\n",
    "        if not constraints:\n",
    "            constraints = ((-1,-1),)\n",
    "\n",
    "        data = [word_parts+list(constraint) for constraint in constraints]\n",
    "\n",
    "        word_enc = self.features_encoder.transform(data).astype(int)\n",
    "        \n",
    "        if len(constraints)==1:\n",
    "\n",
    "            pred = self.nbc_clf.predict(word_enc)[0]\n",
    "            \n",
    "        else:\n",
    "                        \n",
    "            pred = self.nbc_clf.predict_proba(word_enc).mean(0).argmax()\n",
    "        \n",
    "        rule = self.rules_list[pred]\n",
    "    \n",
    "        if rule=='-':\n",
    "            return None\n",
    "        else:\n",
    "            seq_to_remove,seq_to_add = rule\n",
    "            return re.sub(f'{seq_to_remove}$',seq_to_add,word)\n",
    "            \n",
    "        return None\n",
    "\n",
    "class CategoricalNaiveBayes():\n",
    "\n",
    "    def __init__(self, kappa=2, epsilon=1e-20):\n",
    "        \n",
    "        self.kappa = kappa\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def _compute_priors_logprobs(self, y):\n",
    "\n",
    "        priors_probs = [class_counts/len(y) for class_counts in self.class_counts]\n",
    "\n",
    "        self.priors_logprobs = np.log(priors_probs)\n",
    "        \n",
    "    def _compute_loglikelihood(self, X, y):\n",
    "        \n",
    "        feature_counts = {feature_idx:np.zeros((self.n_categories[feature_idx]+2,self.n_classes)) for feature_idx in range(self.n_features)}\n",
    "        \n",
    "        for features, class_idx in zip(X, y):\n",
    "            \n",
    "            for feature_idx,feature_value in enumerate(features):\n",
    "                \n",
    "                feature_counts[feature_idx][feature_value,class_idx] += 1\n",
    "\n",
    "        loglikelihood = {feature_idx:np.zeros((self.n_categories[feature_idx]+2,self.n_classes)) for feature_idx in range(self.n_features)}\n",
    "\n",
    "        for feature_idx in range(self.n_features):\n",
    "            loglikelihood[feature_idx] = np.log((feature_counts[feature_idx]+self.epsilon)\n",
    "                                                          / (np.repeat(self.class_counts[None,...], self.n_categories[feature_idx]+2, axis=0)\n",
    "                                                            + self.kappa*self.epsilon))\n",
    "\n",
    "            loglikelihood[feature_idx][-1,:] = 0\n",
    "\n",
    "        self.loglikelihood = loglikelihood\n",
    "\n",
    "        \n",
    "    def fit(self, X_train, y_train, priors_logprobs=None):\n",
    "\n",
    "        counter = Counter(y_train)\n",
    "        \n",
    "        class_ids, class_counts = zip(*sorted(counter.items()))\n",
    "        \n",
    "        self.class_counts = np.array(class_counts)\n",
    "        self.n_classes = np.max(class_ids)+1\n",
    "\n",
    "        self.n_features = X_train.shape[1]\n",
    "        self.n_categories = X_train.max(axis=0)\n",
    "\n",
    "        if priors_logprobs is None:\n",
    "            self._compute_priors_logprobs(y_train)\n",
    "        else:\n",
    "            self.priors_logprobs = priors_logprobs\n",
    "\n",
    "        self._compute_loglikelihood(X_train, y_train)\n",
    "\n",
    "    def _get_bayes_numerator(self, X):\n",
    "\n",
    "        n_samples = X.shape[0]\n",
    "\n",
    "        sample_loglikelihood = np.zeros((n_samples,self.n_features,self.n_classes))\n",
    "\n",
    "        for feature_idx in range(self.n_features):\n",
    "            \n",
    "            sample_loglikelihood[:,feature_idx,:] = self.loglikelihood[feature_idx][X[:,feature_idx]] #N_samplesxN_classes\n",
    "\n",
    "        numerator = sample_loglikelihood.sum(axis=1)  + self.priors_logprobs[None,...]\n",
    "\n",
    "        return numerator\n",
    "            \n",
    "    def predict_proba(self, X):\n",
    "\n",
    "        numerator = np.exp(self._get_bayes_numerator(X))\n",
    "        \n",
    "        probs = numerator/numerator.sum(axis=1,keepdims=True)\n",
    "                            \n",
    "        return probs\n",
    "\n",
    "    def predict(self, X):\n",
    "\n",
    "        predicted_class_ids = self._get_bayes_numerator(X).argmax(1)\n",
    "\n",
    "        return predicted_class_ids\n",
    "        \n",
    "    def score(self, X, y):\n",
    "\n",
    "        y_pred = self.predict(X)\n",
    "\n",
    "        return (y_pred==np.array(y)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3d26b79-4d9e-4d7d-8d2b-10983c3b1c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NounsStatRules():\n",
    "\n",
    "    def __init__(self, path):\n",
    "\n",
    "        with open(path,'rb') as f:\n",
    "            \n",
    "            data = pickle.load(f)\n",
    "            \n",
    "            self.rules_dict = data['rules_dict']\n",
    "            self.n_last = data['n_last']\n",
    "            \n",
    "    def __call__(self,word):\n",
    "\n",
    "        for idx in range(-self.n_last,0):\n",
    "            rule =  self.rules_dict[f'last_{abs(idx)}'].get(word[idx:],None)\n",
    "            if rule:\n",
    "                seq_to_remove,seq_to_add = rule\n",
    "                return re.sub(f'{seq_to_remove}$',seq_to_add,word)\n",
    "\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b304527-c0d6-424c-948b-56a3371c5b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GLemma():\n",
    "\n",
    "    \"\"\"Wiktionary-based German lemmatizer.\n",
    "\n",
    "    Provides a lemma for a given word given the POS tag:\n",
    "    NOUN, VERB, ADJ, ADV.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    use_nouns_nbc : bool, default=False\n",
    "        Use Naive Bayes classifier for unknown nouns.\n",
    "        Slow (not suitable for annotating large corpora), but precise.\n",
    "\n",
    "    nouns_statrules_acc : int, default=95\n",
    "        Accuracy for statistical tables, can be (95, 99, 100) \n",
    "        When use_nouns_nbc=False, statistical tables are used for unknown nouns to get a lemma based on the ending.\n",
    "\n",
    "    guess_adj_lemmas : bool, default=True\n",
    "        Guess adjective lemmas based on most common endings\n",
    "\n",
    "    wordfreq_csv : str, default=None\n",
    "        A file with approximate word frequencies. \n",
    "        The file must have 2 tab-separated columns for words and their number of occurrences in a corpus.\n",
    "        When multiple lemmas for a given word form are possible, the most frequent lemma is taken.\n",
    "        Does not have to be a lemma list.\n",
    "        \n",
    "    \n",
    "    Examples\n",
    "    --------\n",
    "    >>> lemmatizer = GLemma('./data', \n",
    "                    wordfreq_csv='data/third-party/FrequencyWords/content/2018/de/de_full.txt')\n",
    "    >>> lemmatizer('vermalt','VERB')\n",
    "    'vermalen'\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    If a verb prefix is separated in the sentence, it should be attached to the root before the lemmatization:\n",
    "    Ich hole dich ab --> lemmatizer('abhole','VERB')\n",
    "\n",
    "    By setting nouns_statrules_acc=100, use_nouns_nbc=False,guess_adj_lemmas=False, and wordfreq_csv=None, the lemmatizer only returns\n",
    "    lemmas that it's 100% sure about.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, lemmatizer_data_path=Path('../glemma/data/'),use_nouns_nbc=False, nouns_statrules_acc = 95, guess_adj_lemmas=True, wordfreq_csv=None):\n",
    "\n",
    "        lemmatizer_data_path = Path(os.getcwd()+'/../glemma/data')\n",
    "        #lemmatizer_data_path = Path(__file__).parent.resolve() / 'data'\n",
    "        \n",
    "        with open(lemmatizer_data_path / 'vocab.json', 'rt', encoding='UTF-8') as json_file:\n",
    "            self.vocab = json.load(json_file)\n",
    "\n",
    "        if use_nouns_nbc:\n",
    "            self.nouns_nbc = NounsNBC(lemmatizer_data_path / 'nouns-nbc-top100.pickle')\n",
    "        else:\n",
    "            self.nouns_nbc = None\n",
    "\n",
    "        self.nouns_stat_rules = NounsStatRules(lemmatizer_data_path / f'nouns_stat_rules-{nouns_statrules_acc}.pickle')\n",
    "    \n",
    "        if wordfreq_csv:\n",
    "            self.wordfreq = pd.read_csv(wordfreq_csv, sep=' ', names=['word','freq'])\n",
    "            self.wordfreq.word = self.wordfreq.word.str.lower()\n",
    "            self.wordfreq = self.wordfreq.set_index('word').freq.sort_values(ascending=False) #sort by frequency\n",
    "            self.wordfreq = self.wordfreq.to_dict()\n",
    "        else:\n",
    "            self.wordfreq = None\n",
    "\n",
    "        self.guess_adj_lemmas = guess_adj_lemmas\n",
    "\n",
    "    def get_most_frequent_word(self, wordlist):\n",
    "        '''\n",
    "        Get the most frequent word out of wordlist\n",
    "        '''\n",
    "        \n",
    "        if self.wordfreq:\n",
    "            freqs = [self.wordfreq.get(word, np.nan) for word in wordlist]\n",
    "            if all(np.isnan(freqs)):\n",
    "                return None\n",
    "            else:\n",
    "                #if at least one word in the wordrank dictionary\n",
    "                return wordlist[np.nanargmax(freqs)]\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def get_noun_constraints(self, spacy_token):\n",
    "        '''\n",
    "        Get noun constraints in the form (genus, declination) based on the preceeding articles or preposition\n",
    "        e.g. if the noun preceeded by 'durchs' the constraint is ('n','Akkusativ Singular')\n",
    "        multiple constraints are possible\n",
    "        '''\n",
    "\n",
    "        if spacy_token is None:\n",
    "            return None\n",
    "        \n",
    "        ancestors_lemmas = [x.text.lower() for x in spacy_token.ancestors] #hope to find prepositions here, can be fused with articles, e.g. im, durchs\n",
    "        for ancestors_lemma in ancestors_lemmas:\n",
    "            ancestors_constraints = prep_constraints.get(ancestors_lemma, None)\n",
    "            if ancestors_constraints:\n",
    "                return ancestors_constraints\n",
    "            \n",
    "        childeren_lemmas = [x.text.lower() for x in spacy_token.children] #hope to find determiners here, e.g. der, diese, etc.\n",
    "        for childeren_lemma in childeren_lemmas:\n",
    "            base_determiner = get_base_determiner(childeren_lemma)#convert determiners to canonical form\n",
    "            if base_determiner: \n",
    "                return article_constraints[base_determiner]\n",
    "                \n",
    "        return None\n",
    "\n",
    "    def filter_verb_lemmas(self, lemmas, spacy_token):\n",
    "\n",
    "        if spacy_token is None:\n",
    "            return None\n",
    "            \n",
    "        if spacy_token.head.lemma_ in ('haben','sein'):\n",
    "            #Perfekt suspected\n",
    "            n_hilfsverb = len(set([y for x in lemmas for y in x['via']]))\n",
    "            if n_hilfsverb>1: #do we really have to choose between sein and haben?\n",
    "                lemmas = [x for x in lemmas if x['connection']=='Partizip II' \n",
    "                          #token head should match the auxiliary verb\n",
    "                                    and spacy_token.head.lemma_ in x['via']]\n",
    "        elif spacy_token.head.lemma_=='werden':\n",
    "            #werden is an auxiliary verb for Passiv or Futur, \n",
    "            #the wordform should be Partizip II (Passiv) or the same as lemma (Futur)\n",
    "            lemmas = [x for x in lemmas if x['connection']=='Partizip II' \n",
    "                                or x['lemma']==spacy_token.text.lower()]\n",
    "        else:\n",
    "            #no evidence for Perfekt or Passiv, so the wordform can't be Partizip II\n",
    "            lemmas = [x for x in lemmas if not x['connection']=='Partizip II']\n",
    "\n",
    "        return lemmas\n",
    "            \n",
    "    def get_word_lemma(self, word, pos, spacy_token=None):\n",
    "\n",
    "        lemmas = self.vocab[pos].get(word, None)   \n",
    "\n",
    "        if not lemmas:\n",
    "            #maybe old orthography? try to replace ß with ss at the end of the stem \n",
    "            newform=re.sub(r'ß($|es$|t?en$|t?e$|t?e?t$|t?est$)',r'ss\\1', word) \n",
    "            lemmas = self.vocab[pos].get(newform, None)   \n",
    "\n",
    "        if not lemmas:\n",
    "            return None\n",
    "            \n",
    "        n_unique_lemmas = len(set([x['lemma'] for x in lemmas])) #count unique lemmas, e.g. 'konzentriert' will have 2 records: one for the infinitive and one for the Partizip II\n",
    "        \n",
    "        if n_unique_lemmas>1 and spacy_token:\n",
    "\n",
    "            #multiple lemmas possible for this wordform\n",
    "            #use Spacy dependency parcer to reduce the possibilities\n",
    "\n",
    "            if pos=='N':\n",
    "                #look for noun constraints, e.g. a related article imposes a particular declination, thus a particular wordform\n",
    "                constraints = self.get_noun_constraints(spacy_token)\n",
    "                \n",
    "                if constraints:\n",
    "                    lemmas = [lemma for lemma in lemmas if (lemma['genus'],lemma['declination']) in constraints]\n",
    "                \n",
    "            elif pos=='V':\n",
    "                lemmas = self.filter_verb_lemmas(lemmas, spacy_token)\n",
    "                \n",
    "        lemmas = list(set([x['lemma'] for x in lemmas])) #remove all meta info, take unique words\n",
    "        \n",
    "        if not lemmas:\n",
    "            return None\n",
    "            \n",
    "        elif len(lemmas)>1:\n",
    "            #get most frequent lemma, can be very imprecise for frequency dictionaries computed on small datasets \n",
    "            return self.get_most_frequent_word(lemmas)\n",
    "            \n",
    "        else:\n",
    "            return lemmas[0]\n",
    "\n",
    "    def __call__(self, word=None, pos=None, spacy_token=None):\n",
    "\n",
    "        lemma = None\n",
    "\n",
    "        if not word:\n",
    "            word, pos = spacy_token.text, spacy_token.pos_\n",
    "\n",
    "        word = word.lower()\n",
    "\n",
    "        for pos_tag in ('N','V','ADJ','ADV'):\n",
    "            #conver pos to unified pos_tag\n",
    "            if pos.startswith(pos_tag):\n",
    "                pos = pos_tag\n",
    "\n",
    "        if not pos in ('N','V','ADJ','ADV'):\n",
    "            #lemmatizer works only for nouns, verbs, adjectives, and adverbs\n",
    "            return None\n",
    "\n",
    "        if pos=='ADV':\n",
    "            #first treat adverb as an adjective\n",
    "            #because Wiktionary dictionary for adverbs is incomplete and almost any adjective in German can be used as an adverb\n",
    "            lemma = self.get_word_lemma(word, 'ADJ')\n",
    "\n",
    "        if not lemma:\n",
    "            lemma = self.get_word_lemma(word, pos, spacy_token=spacy_token)\n",
    "\n",
    "        if not lemma:\n",
    "            if pos=='N':\n",
    "                if self.nouns_nbc:\n",
    "                    #Naive Bayes classifier: slow, but precise\n",
    "                    constraints = self.get_noun_constraints(spacy_token)\n",
    "                    lemma = self.nouns_nbc(word, constraints)\n",
    "                else:\n",
    "                    #statistical tables to predict lemma based on endings\n",
    "                    lemma = self.nouns_stat_rules(word)\n",
    "            elif pos=='V':\n",
    "                    #look for known lemma at the end of the word\n",
    "                    lemma = get_verb_lemma_fwdsearch(word, lambda x:self.get_word_lemma(x, pos, spacy_token=spacy_token), \n",
    "                                                     use_longest_subword=True)\n",
    "            elif pos in 'ADJ' and self.guess_adj_lemmas:\n",
    "                    #assume most common adjective endings\n",
    "                    lemma = re.sub('e[rsnm]?$','',word)\n",
    "                \n",
    "        if pos=='N' and lemma:\n",
    "            #noun lemmas starts with a capital\n",
    "            lemma = lemma.title()\n",
    "            \n",
    "        return lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1569d82a-c9cb-42a4-995b-aeb96b7a2c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = GLemma(\n",
    "                    wordfreq_csv='../glemma/data/third-party/FrequencyWords/content/2018/de/de_full.txt',\n",
    "                    use_nouns_nbc=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "992e2884-8e9e-49f3-bdf3-eea5e8fec11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spacy_model(language='en'):\n",
    "\n",
    "    import spacy\n",
    "\n",
    "    language = {\n",
    "        'en': 'en_core_web_lg',\n",
    "        'fr': 'french',\n",
    "        'de': 'de_core_news_lg',\n",
    "    }[language]\n",
    "    return spacy.load(language) \n",
    "\n",
    "spacy_model = get_spacy_model('de')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4a6640-3dce-4710-91e5-f9fe0e7a2556",
   "metadata": {},
   "source": [
    "## Test TIGER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1fed9cd5-e63e-4c03-9263-9a0fc0ca5731",
   "metadata": {},
   "outputs": [],
   "source": [
    "tiger_dataset = '../glemma/data/third-party/tiger_release_aug07.corrected.16012013.xml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "537b4358-a37c-41ea-a064-684bb00c2968",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tiger():\n",
    "    \n",
    "    sentences = []\n",
    "\n",
    "    with open(tiger_dataset,'r', encoding='iso-8859-15') as f:\n",
    "        while True:\n",
    "            line = f.readline()\n",
    "            while not '<terminals>' in line:\n",
    "                line = f.readline()\n",
    "                if '</corpus>' in line:\n",
    "                    return sentences\n",
    "            words = []\n",
    "            while not '</terminals>' in line:\n",
    "                line = f.readline()\n",
    "                s = re.search(r'word=\"(\\w+)\" lemma=\"(\\w+)\" pos=\"(\\w+)\"',line)\n",
    "                if s:\n",
    "                    words.append(s.groups(0))\n",
    "            if len(words)>0:\n",
    "                sentences.append(words)\n",
    "\n",
    "tiger_sentences = read_tiger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5ed49b98-523b-4d52-87f5-4e9dbc8cfc9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49827"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tiger_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "52de2d12-a642-4293-9a61-89dae7b471f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from HanTa import HanoverTagger as ht\n",
    "\n",
    "tagger_de = ht.HanoverTagger('morphmodel_ger.pgz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1418bbe6-e3e6-4499-8f78-6365ea940079",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('frei', 'ADJ(D)')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger_de.analyze('frei')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "24ad3521-e7c2-4838-abd0-8696e9c863f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos(hanta_pos):\n",
    "    if hanta_pos.startswith('ADJ'):\n",
    "        return 'ADJ'\n",
    "    if hanta_pos.startswith('ADV'):\n",
    "        return 'ADV'\n",
    "    if hanta_pos.startswith('N'):\n",
    "        return 'NOUN'\n",
    "    if hanta_pos.startswith('V'):\n",
    "        return 'VERB'\n",
    "    return hanta_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "9e278e32-72ba-476a-a0f4-76f48afb4ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3999\n"
     ]
    }
   ],
   "source": [
    "tiger_res = []\n",
    "\n",
    "for idx,sentence in enumerate(tiger_sentences):\n",
    "    words, lemmas, pos = zip(*sentence)\n",
    "    text = ' '.join(words)\n",
    "    doc = spacy_model(text)\n",
    "    if len(doc)==len(lemmas):\n",
    "        for token, tiger_word, tiger_lemma, tiger_pos in zip(doc,words,lemmas,pos):\n",
    "             if tiger_word == token.text:\n",
    "                 hanta_lemma, hanta_pos = tagger_de.analyze(token.text)\n",
    "                 pos = get_pos(hanta_pos)\n",
    "                 if pos in ('NOUN','ADJ','ADV','VERB') and token.pos_!='PROPN':\n",
    "                   lemma = lemmatizer(token.text, pos)\n",
    "                   if pos=='NOUN':\n",
    "                       tiger_lemma = tiger_lemma.title()\n",
    "                   tiger_res.append((text,tiger_word, tiger_pos, tiger_lemma, hanta_lemma, pos, token.lemma_, lemma))\n",
    "    if (idx+1)%4000==0:\n",
    "        print(idx)\n",
    "        break\n",
    "\n",
    "tiger_res = pd.DataFrame(tiger_res, columns = ['sentence','word','tiger_pos','tiger_lemma','hanta_lemma','pos','spacy_lemma','pred_lemma'])\n",
    "\n",
    "tiger_res['correct'] = tiger_res.pred_lemma == tiger_res.tiger_lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f59f86e5-a844-4555-b3a3-91886d3c80b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tiger_res = tiger_res[~tiger_res.tiger_lemma.str.endswith('ß')]\n",
    "#tiger_res = tiger_res[~tiger_res.tiger_lemma.apply(lambda x:x.lower()==x)]\n",
    "#tiger_res = tiger_res[~tiger_res.tiger_lemma.apply(lambda x:x.upper()==x)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8fa67c54-d4ca-4881-87d2-4248287f21e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pos\n",
       "ADJ     0.894698\n",
       "ADV     0.917372\n",
       "NOUN    0.958857\n",
       "VERB    0.985467\n",
       "Name: correct, dtype: float64"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tiger_res.groupby('pos').correct.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "fb7eab87-3470-44db-894c-f167c833d8d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pos\n",
       "ADJ     0.904536\n",
       "ADV     0.998048\n",
       "NOUN    0.982949\n",
       "VERB    0.990050\n",
       "Name: hanta_correct, dtype: float64"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tiger_res['hanta_correct'] = tiger_res.hanta_lemma == tiger_res.tiger_lemma\n",
    "tiger_res.groupby('pos').hanta_correct.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "4bcf88cb-1433-4e73-b200-b9625f704bf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>word</th>\n",
       "      <th>tiger_lemma</th>\n",
       "      <th>pred_lemma</th>\n",
       "      <th>hanta_lemma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>506</th>\n",
       "      <td>Während die neuen Tiger Asiens wirtschaftlich vorbeizogen kam der Gigant dessen Geschäftsleute nur im Ausland demonstrieren konnten was in ihnen steckt nicht voran</td>\n",
       "      <td>Geschäftsleute</td>\n",
       "      <td>Geschäftsleute</td>\n",
       "      <td>Geschäftsmann</td>\n",
       "      <td>Geschäftsleute</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>824</th>\n",
       "      <td>FR Auf die Wahlerfolge der rechtsradikalen Parteien haben die Etablierten in den Parlamenten und viele Medien bisher mit einer Strategie gezielter Ignoranz reagiert</td>\n",
       "      <td>Medien</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Media</td>\n",
       "      <td>Medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2086</th>\n",
       "      <td>Ungefähr die Hälfte des Schadens soll durch Betriebsfremde verursacht sein</td>\n",
       "      <td>Schadens</td>\n",
       "      <td>Schaden</td>\n",
       "      <td>Schade</td>\n",
       "      <td>Schaden</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3184</th>\n",
       "      <td>Sie benutzte die Lumpen Asozialen Dealer Messerstecher Schmarotzer und Geier für die eigenen Ziele</td>\n",
       "      <td>Lumpen</td>\n",
       "      <td>Lump</td>\n",
       "      <td>Lumpen</td>\n",
       "      <td>Lump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5148</th>\n",
       "      <td>Der malerische Flecken lebt wie viele Dörfer von der Landwirtschaft</td>\n",
       "      <td>Flecken</td>\n",
       "      <td>Flecken</td>\n",
       "      <td>Fleck</td>\n",
       "      <td>Flecken</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5891</th>\n",
       "      <td>Fachleute sind sich sicher</td>\n",
       "      <td>Fachleute</td>\n",
       "      <td>Fachleute</td>\n",
       "      <td>Fachmann</td>\n",
       "      <td>Fachleute</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6491</th>\n",
       "      <td>Rene Jäggi hatte sie dem Franzosen vorgelegt</td>\n",
       "      <td>Franzosen</td>\n",
       "      <td>Franzose</td>\n",
       "      <td>Franzosen</td>\n",
       "      <td>Franzose</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7729</th>\n",
       "      <td>Doch die Russen erschienen nicht zum vereinbarten Treffen</td>\n",
       "      <td>Russen</td>\n",
       "      <td>Russe</td>\n",
       "      <td>Russ</td>\n",
       "      <td>Russe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8969</th>\n",
       "      <td>Schlechtes Orientierungsvermögen zeigten zwei dänische Seeleute beim Auffinden ihres Schiffes</td>\n",
       "      <td>Seeleute</td>\n",
       "      <td>Seeleute</td>\n",
       "      <td>Seemann</td>\n",
       "      <td>Seeleute</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9542</th>\n",
       "      <td>Dazu sollten sich die Europäer und die Asiaten gleichermaßen bemühen die nach Nippon auszuweiten heißt es in einer gemeinsamen Erklärung nach Abschluß eines Gipfeltreffens in London</td>\n",
       "      <td>Asiaten</td>\n",
       "      <td>Asiat</td>\n",
       "      <td>Asiate</td>\n",
       "      <td>Asiat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9852</th>\n",
       "      <td>Der neue Regierungschef hat die Sanierung der Staatsfinanzen zum wichtigsten Ziel seiner Regierungsarbeit erklärt</td>\n",
       "      <td>Staatsfinanzen</td>\n",
       "      <td>Staatsfinanz</td>\n",
       "      <td>Staatsfinanzen</td>\n",
       "      <td>Staatsfinanz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10555</th>\n",
       "      <td>Beim Verbraucherpreisanstieg sagen sie in Westeuropa nach einer Rate von Prozent einen Rückgang auf Prozent in diesem und auf Prozent im nächsten Jahr voraus</td>\n",
       "      <td>Rate</td>\n",
       "      <td>Rate</td>\n",
       "      <td>Rat</td>\n",
       "      <td>Rate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12905</th>\n",
       "      <td>Trotz der Bemühungen des Unternehmens die Freigabe des Präparats als Mittel gegen viele Erkrankungen einschließlich Aids zu erwirken ist Virazole in den USA nur in Sprühform gegen Erkrankungen der Atemwege bei Säuglingen zugelassen</td>\n",
       "      <td>Aids</td>\n",
       "      <td>Aids</td>\n",
       "      <td>Aid</td>\n",
       "      <td>Aids</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16646</th>\n",
       "      <td>Wenn der deutsche Regierungschef die Ankündigung seines Sherpas wie die mit den Gipfelvorbereitungen beauftragten Vertrauensleute der Regierungschefs heißen in diesem Falle Finanzstaatssekretär Horst Köhler wahrgemacht hat ist Tacheles geredet worden im Hotel Vier Jahreszeiten</td>\n",
       "      <td>Vertrauensleute</td>\n",
       "      <td>Vertrauensleute</td>\n",
       "      <td>Vertrauensmann</td>\n",
       "      <td>Vertrauensleute</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16654</th>\n",
       "      <td>Wenn der deutsche Regierungschef die Ankündigung seines Sherpas wie die mit den Gipfelvorbereitungen beauftragten Vertrauensleute der Regierungschefs heißen in diesem Falle Finanzstaatssekretär Horst Köhler wahrgemacht hat ist Tacheles geredet worden im Hotel Vier Jahreszeiten</td>\n",
       "      <td>Tacheles</td>\n",
       "      <td>Tacheles</td>\n",
       "      <td>Tachel</td>\n",
       "      <td>Tacheles</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16792</th>\n",
       "      <td>Dafür könnte Kanadas Premier Brian Mulroney der nach dem Primus aus Tokio vom deutschen Regierungschef empfangen wurde des Wohlwollens seines Gastgebers sicher gewesen sein weil beide Länder mit ihren wirtschaftlichen Daten derzeit im schlechten Mittelmaß versinken mit den Defiziten in ihren Leistungsbilanzen ebenso wie in ihren Staatsbudgets</td>\n",
       "      <td>Daten</td>\n",
       "      <td>Datum</td>\n",
       "      <td>Daten</td>\n",
       "      <td>Datum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17883</th>\n",
       "      <td>Höhere Einkünfte werden mit dem persönlichen Einkommensteuersatz von maximal 53 Prozent belegt</td>\n",
       "      <td>Einkünfte</td>\n",
       "      <td>Einkünfte</td>\n",
       "      <td>Einkunft</td>\n",
       "      <td>Einkünfte</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18594</th>\n",
       "      <td>Dann mußte sich der König von Mitgliedern seiner Umgebung 5000 Peseten 80 Mark leihen weil er ein Abzeichen in Form eines Flamingos kaufen wollte</td>\n",
       "      <td>Peseten</td>\n",
       "      <td>Pesete</td>\n",
       "      <td>Peseta</td>\n",
       "      <td>Pesete</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18774</th>\n",
       "      <td>Bei Verladearbeiten in einer Halle sei eines der Fässer mit dem Konzentrat abgestürzt hieß es</td>\n",
       "      <td>Halle</td>\n",
       "      <td>Halle</td>\n",
       "      <td>Hall</td>\n",
       "      <td>Halle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18790</th>\n",
       "      <td>Da die Beschäftigten Schutzmasken getragen hätten sei kein Schaden entstanden</td>\n",
       "      <td>Schaden</td>\n",
       "      <td>Schaden</td>\n",
       "      <td>Schade</td>\n",
       "      <td>Schaden</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20267</th>\n",
       "      <td>Der bei dem die Gründerfamilie das Sagen hat macht zwar den Großteil seines Umsatzes von zuletzt zwei Milliarden Mark inzwischen mit Getränken zum Beispiel Biere der Marken Tucher Henninger EKU Jever verfügt aber auch über ein lukratives Fleischgeschäft 15 Prozent Erlösanteil unter der Marke Marox</td>\n",
       "      <td>Marken</td>\n",
       "      <td>Marke</td>\n",
       "      <td>Mark</td>\n",
       "      <td>Mark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20497</th>\n",
       "      <td>Nach einigen Warnungen über Lautsprecher schlugen Polizisten die Seitenscheibe des ersten Lastwagens in der Kolonne ein um die Bremsen zu lösen</td>\n",
       "      <td>Bremsen</td>\n",
       "      <td>Bremse</td>\n",
       "      <td>Bremsen</td>\n",
       "      <td>Bremse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23111</th>\n",
       "      <td>Wenn in einem Strafprozeß in Großbritannien der Zeuge der Krone auftritt muß der Richter die Geschworenen auf die Gefahren dieses Beweismittels hinweisen</td>\n",
       "      <td>Zeuge</td>\n",
       "      <td>Zeuge</td>\n",
       "      <td>Zeug</td>\n",
       "      <td>Zeuge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23308</th>\n",
       "      <td>Was Euer Ehren ist denn nun angemessen oder geht es vor dem deutschen Kadi wirklich nur um ein Urteil und nicht um Gerechtigkeit</td>\n",
       "      <td>Ehren</td>\n",
       "      <td>Ehren</td>\n",
       "      <td>Ehre</td>\n",
       "      <td>Ehre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24752</th>\n",
       "      <td>Die NRC waren bei den Haussa im islamischen Norden aber auch bei den christlichen Ibos im Südwesten erfolgreich</td>\n",
       "      <td>Ibos</td>\n",
       "      <td>Ibos</td>\n",
       "      <td>Ibo</td>\n",
       "      <td>Ibos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26193</th>\n",
       "      <td>Nach Ansicht von John Hontelez Präsident des internationalen Friends of the Earth kann bei der Stillegung der 39 riskantesten Meiler die Hälfte der Atomkraftleistung in Höhe von 12 000 Megawatt sehr rasch durch schlüsselfertige Containerheizkraftwerke für rund zehn Millionen Mark pro Einheit ersetzt werden die verbleibenden 50 Prozent durch Energieeinsparung in Industrie Haushalten und Verkehr</td>\n",
       "      <td>Friends</td>\n",
       "      <td>Friends</td>\n",
       "      <td>Friend</td>\n",
       "      <td>Friends</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26388</th>\n",
       "      <td>Dabei lassen sich die Initiatoren des Appells von folgendem Grundgedanken leiten</td>\n",
       "      <td>Grundgedanken</td>\n",
       "      <td>Grundgedanken</td>\n",
       "      <td>Grundgedanke</td>\n",
       "      <td>Grundgedanken</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27860</th>\n",
       "      <td>Unter dem Strich blieb bei leicht verbesserter Spanne ein um rund 600 Millionen Mark höherer Rohertrag übrig und das im Vergleich zum ausgezeichneten Jahr 1990</td>\n",
       "      <td>Spanne</td>\n",
       "      <td>Spanne</td>\n",
       "      <td>Spann</td>\n",
       "      <td>Spanne</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31146</th>\n",
       "      <td>Zuviel Spezialwissen soll dafür aus den Prüfungskatalogen gestrichen und in und Vertiefungsstudiengängen angeboten werden</td>\n",
       "      <td>Prüfungskatalogen</td>\n",
       "      <td>Prüfungskatalog</td>\n",
       "      <td>Prüfungskataloge</td>\n",
       "      <td>Prüfungskatalog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32241</th>\n",
       "      <td>Der Gewerkschaftsvorsitzende Klaus Zwickel sagte am Samstag zum Abschluß des Gewerkschaftstages in Berlin Wir wollen mehr Arbeitsplätze und mehr Zeitsouveränität durch Freizeitausgleich für Mehrarbeit und Arbeitszeitkonten</td>\n",
       "      <td>Gewerkschaftsvorsitzende</td>\n",
       "      <td>Gewerkschaftsvorsitzende</td>\n",
       "      <td>Gewerkschaftsvorsitzender</td>\n",
       "      <td>Gewerkschaftsvorsitzende</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                           sentence  \\\n",
       "506                                                                                                                                                                                                                                             Während die neuen Tiger Asiens wirtschaftlich vorbeizogen kam der Gigant dessen Geschäftsleute nur im Ausland demonstrieren konnten was in ihnen steckt nicht voran   \n",
       "824                                                                                                                                                                                                                                            FR Auf die Wahlerfolge der rechtsradikalen Parteien haben die Etablierten in den Parlamenten und viele Medien bisher mit einer Strategie gezielter Ignoranz reagiert   \n",
       "2086                                                                                                                                                                                                                                                                                                                                     Ungefähr die Hälfte des Schadens soll durch Betriebsfremde verursacht sein   \n",
       "3184                                                                                                                                                                                                                                                                                                             Sie benutzte die Lumpen Asozialen Dealer Messerstecher Schmarotzer und Geier für die eigenen Ziele   \n",
       "5148                                                                                                                                                                                                                                                                                                                                            Der malerische Flecken lebt wie viele Dörfer von der Landwirtschaft   \n",
       "5891                                                                                                                                                                                                                                                                                                                                                                                     Fachleute sind sich sicher   \n",
       "6491                                                                                                                                                                                                                                                                                                                                                                   Rene Jäggi hatte sie dem Franzosen vorgelegt   \n",
       "7729                                                                                                                                                                                                                                                                                                                                                      Doch die Russen erschienen nicht zum vereinbarten Treffen   \n",
       "8969                                                                                                                                                                                                                                                                                                                  Schlechtes Orientierungsvermögen zeigten zwei dänische Seeleute beim Auffinden ihres Schiffes   \n",
       "9542                                                                                                                                                                                                                          Dazu sollten sich die Europäer und die Asiaten gleichermaßen bemühen die nach Nippon auszuweiten heißt es in einer gemeinsamen Erklärung nach Abschluß eines Gipfeltreffens in London   \n",
       "9852                                                                                                                                                                                                                                                                                              Der neue Regierungschef hat die Sanierung der Staatsfinanzen zum wichtigsten Ziel seiner Regierungsarbeit erklärt   \n",
       "10555                                                                                                                                                                                                                                                 Beim Verbraucherpreisanstieg sagen sie in Westeuropa nach einer Rate von Prozent einen Rückgang auf Prozent in diesem und auf Prozent im nächsten Jahr voraus   \n",
       "12905                                                                                                                                                                       Trotz der Bemühungen des Unternehmens die Freigabe des Präparats als Mittel gegen viele Erkrankungen einschließlich Aids zu erwirken ist Virazole in den USA nur in Sprühform gegen Erkrankungen der Atemwege bei Säuglingen zugelassen   \n",
       "16646                                                                                                                         Wenn der deutsche Regierungschef die Ankündigung seines Sherpas wie die mit den Gipfelvorbereitungen beauftragten Vertrauensleute der Regierungschefs heißen in diesem Falle Finanzstaatssekretär Horst Köhler wahrgemacht hat ist Tacheles geredet worden im Hotel Vier Jahreszeiten   \n",
       "16654                                                                                                                         Wenn der deutsche Regierungschef die Ankündigung seines Sherpas wie die mit den Gipfelvorbereitungen beauftragten Vertrauensleute der Regierungschefs heißen in diesem Falle Finanzstaatssekretär Horst Köhler wahrgemacht hat ist Tacheles geredet worden im Hotel Vier Jahreszeiten   \n",
       "16792                                                      Dafür könnte Kanadas Premier Brian Mulroney der nach dem Primus aus Tokio vom deutschen Regierungschef empfangen wurde des Wohlwollens seines Gastgebers sicher gewesen sein weil beide Länder mit ihren wirtschaftlichen Daten derzeit im schlechten Mittelmaß versinken mit den Defiziten in ihren Leistungsbilanzen ebenso wie in ihren Staatsbudgets   \n",
       "17883                                                                                                                                                                                                                                                                                                                Höhere Einkünfte werden mit dem persönlichen Einkommensteuersatz von maximal 53 Prozent belegt   \n",
       "18594                                                                                                                                                                                                                                                             Dann mußte sich der König von Mitgliedern seiner Umgebung 5000 Peseten 80 Mark leihen weil er ein Abzeichen in Form eines Flamingos kaufen wollte   \n",
       "18774                                                                                                                                                                                                                                                                                                                 Bei Verladearbeiten in einer Halle sei eines der Fässer mit dem Konzentrat abgestürzt hieß es   \n",
       "18790                                                                                                                                                                                                                                                                                                                                 Da die Beschäftigten Schutzmasken getragen hätten sei kein Schaden entstanden   \n",
       "20267                                                                                                    Der bei dem die Gründerfamilie das Sagen hat macht zwar den Großteil seines Umsatzes von zuletzt zwei Milliarden Mark inzwischen mit Getränken zum Beispiel Biere der Marken Tucher Henninger EKU Jever verfügt aber auch über ein lukratives Fleischgeschäft 15 Prozent Erlösanteil unter der Marke Marox   \n",
       "20497                                                                                                                                                                                                                                                               Nach einigen Warnungen über Lautsprecher schlugen Polizisten die Seitenscheibe des ersten Lastwagens in der Kolonne ein um die Bremsen zu lösen   \n",
       "23111                                                                                                                                                                                                                                                     Wenn in einem Strafprozeß in Großbritannien der Zeuge der Krone auftritt muß der Richter die Geschworenen auf die Gefahren dieses Beweismittels hinweisen   \n",
       "23308                                                                                                                                                                                                                                                                              Was Euer Ehren ist denn nun angemessen oder geht es vor dem deutschen Kadi wirklich nur um ein Urteil und nicht um Gerechtigkeit   \n",
       "24752                                                                                                                                                                                                                                                                                               Die NRC waren bei den Haussa im islamischen Norden aber auch bei den christlichen Ibos im Südwesten erfolgreich   \n",
       "26193  Nach Ansicht von John Hontelez Präsident des internationalen Friends of the Earth kann bei der Stillegung der 39 riskantesten Meiler die Hälfte der Atomkraftleistung in Höhe von 12 000 Megawatt sehr rasch durch schlüsselfertige Containerheizkraftwerke für rund zehn Millionen Mark pro Einheit ersetzt werden die verbleibenden 50 Prozent durch Energieeinsparung in Industrie Haushalten und Verkehr   \n",
       "26388                                                                                                                                                                                                                                                                                                                              Dabei lassen sich die Initiatoren des Appells von folgendem Grundgedanken leiten   \n",
       "27860                                                                                                                                                                                                                                               Unter dem Strich blieb bei leicht verbesserter Spanne ein um rund 600 Millionen Mark höherer Rohertrag übrig und das im Vergleich zum ausgezeichneten Jahr 1990   \n",
       "31146                                                                                                                                                                                                                                                                                     Zuviel Spezialwissen soll dafür aus den Prüfungskatalogen gestrichen und in und Vertiefungsstudiengängen angeboten werden   \n",
       "32241                                                                                                                                                                                Der Gewerkschaftsvorsitzende Klaus Zwickel sagte am Samstag zum Abschluß des Gewerkschaftstages in Berlin Wir wollen mehr Arbeitsplätze und mehr Zeitsouveränität durch Freizeitausgleich für Mehrarbeit und Arbeitszeitkonten   \n",
       "\n",
       "                           word               tiger_lemma  \\\n",
       "506              Geschäftsleute            Geschäftsleute   \n",
       "824                      Medien                    Medium   \n",
       "2086                   Schadens                   Schaden   \n",
       "3184                     Lumpen                      Lump   \n",
       "5148                    Flecken                   Flecken   \n",
       "5891                  Fachleute                 Fachleute   \n",
       "6491                  Franzosen                  Franzose   \n",
       "7729                     Russen                     Russe   \n",
       "8969                   Seeleute                  Seeleute   \n",
       "9542                    Asiaten                     Asiat   \n",
       "9852             Staatsfinanzen              Staatsfinanz   \n",
       "10555                      Rate                      Rate   \n",
       "12905                      Aids                      Aids   \n",
       "16646           Vertrauensleute           Vertrauensleute   \n",
       "16654                  Tacheles                  Tacheles   \n",
       "16792                     Daten                     Datum   \n",
       "17883                 Einkünfte                 Einkünfte   \n",
       "18594                   Peseten                    Pesete   \n",
       "18774                     Halle                     Halle   \n",
       "18790                   Schaden                   Schaden   \n",
       "20267                    Marken                     Marke   \n",
       "20497                   Bremsen                    Bremse   \n",
       "23111                     Zeuge                     Zeuge   \n",
       "23308                     Ehren                     Ehren   \n",
       "24752                      Ibos                      Ibos   \n",
       "26193                   Friends                   Friends   \n",
       "26388             Grundgedanken             Grundgedanken   \n",
       "27860                    Spanne                    Spanne   \n",
       "31146         Prüfungskatalogen           Prüfungskatalog   \n",
       "32241  Gewerkschaftsvorsitzende  Gewerkschaftsvorsitzende   \n",
       "\n",
       "                      pred_lemma               hanta_lemma  \n",
       "506                Geschäftsmann            Geschäftsleute  \n",
       "824                        Media                    Medium  \n",
       "2086                      Schade                   Schaden  \n",
       "3184                      Lumpen                      Lump  \n",
       "5148                       Fleck                   Flecken  \n",
       "5891                    Fachmann                 Fachleute  \n",
       "6491                   Franzosen                  Franzose  \n",
       "7729                        Russ                     Russe  \n",
       "8969                     Seemann                  Seeleute  \n",
       "9542                      Asiate                     Asiat  \n",
       "9852              Staatsfinanzen              Staatsfinanz  \n",
       "10555                        Rat                      Rate  \n",
       "12905                        Aid                      Aids  \n",
       "16646             Vertrauensmann           Vertrauensleute  \n",
       "16654                     Tachel                  Tacheles  \n",
       "16792                      Daten                     Datum  \n",
       "17883                   Einkunft                 Einkünfte  \n",
       "18594                     Peseta                    Pesete  \n",
       "18774                       Hall                     Halle  \n",
       "18790                     Schade                   Schaden  \n",
       "20267                       Mark                      Mark  \n",
       "20497                    Bremsen                    Bremse  \n",
       "23111                       Zeug                     Zeuge  \n",
       "23308                       Ehre                      Ehre  \n",
       "24752                        Ibo                      Ibos  \n",
       "26193                     Friend                   Friends  \n",
       "26388               Grundgedanke             Grundgedanken  \n",
       "27860                      Spann                    Spanne  \n",
       "31146           Prüfungskataloge           Prüfungskatalog  \n",
       "32241  Gewerkschaftsvorsitzender  Gewerkschaftsvorsitzende  "
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fails_df = tiger_res[(tiger_res.tiger_lemma!=tiger_res.pred_lemma)&(tiger_res.tiger_lemma==tiger_res.spacy_lemma)].drop_duplicates(subset=['word','tiger_lemma'])\n",
    "fails_df[fails_df.pos=='NOUN'].loc[~fails_df.pred_lemma.isna(),['sentence','word','tiger_lemma','pred_lemma','hanta_lemma']].head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af64ab0a-edb0-4595-8fc0-4b9d076272c2",
   "metadata": {},
   "source": [
    "## Test HDT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "df8c2f5b-2c6f-434e-bc1b-059593b4ca05",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdt_dataset = '../glemma/data/third-party/UD_German-HDT/de_hdt-ud-train-a-1.conllu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d07aa9f6-a6aa-48d9-9b49-9e2f13e520ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_hdt():\n",
    "    \n",
    "    sentences = []\n",
    "    words = []\n",
    "\n",
    "    prev_sent_id = ''\n",
    "    \n",
    "    with open(hdt_dataset,'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            if 'sent_id' in line:\n",
    "                sent_id = re.search(r'sent_id = (.+)',line).groups(1)\n",
    "                if sent_id!=prev_sent_id:\n",
    "                    if words:\n",
    "                        sentences.append(words)\n",
    "                    words = []\n",
    "                    prev_sent_id = sent_id\n",
    "            else:\n",
    "                s = re.match(r'[0-9]+\\t([\\w]+)\\t([\\w]+)\\t([\\w]+)',line)\n",
    "                if s:\n",
    "                    words.append(s.groups(0))\n",
    "\n",
    "    return sentences\n",
    "\n",
    "sentences_hdt = read_hdt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "babac098-4bb7-403b-a6b3-411f9f712cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1999\n"
     ]
    }
   ],
   "source": [
    "hdt_res = []\n",
    "\n",
    "for idx,sentence in enumerate(sentences_hdt):\n",
    "    words, lemmas, pos = zip(*sentence)\n",
    "    text = ' '.join(words)\n",
    "    doc = spacy_model(text)\n",
    "    if len(doc)==len(lemmas):\n",
    "        for token, hdt_word, hdt_lemma, hdt_pos in zip(doc,words,lemmas,pos):\n",
    "             if hdt_word == token.text and token.pos_ in ('NOUN','ADJ','ADV','VERB'):\n",
    "                   lemma = lemmatizer(spacy_token=token)\n",
    "                   hanta_lemma= tagger_de.analyze(token.text)[0]\n",
    "                   hdt_res.append((text,hdt_word, hdt_pos, hdt_lemma, hanta_lemma, token.pos_, token.lemma_, lemma))\n",
    "    if (idx+1)%2000==0:\n",
    "        print(idx)\n",
    "        break\n",
    "\n",
    "hdt_res = pd.DataFrame(hdt_res, columns = ['sentence','word','hdt_pos','hdt_lemma', 'hanta_lemma','spacy_pos','spacy_lemma','pred_lemma'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6b2f2270-e0fd-4685-a386-fc471f7be6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdt_res['correct'] = hdt_res.pred_lemma == hdt_res.hdt_lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b226d0fd-30de-4a7b-91e6-03616da25030",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdt_res = hdt_res[~hdt_res.hdt_lemma.str.endswith('ß')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cccdca1b-d687-4c4d-b470-874b4c5eb76d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy_pos\n",
       "ADJ     0.638686\n",
       "ADV     0.831165\n",
       "NOUN    0.743366\n",
       "VERB    0.992151\n",
       "Name: correct, dtype: float64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hdt_res.groupby('spacy_pos').correct.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6b609a38-1f34-4577-93a1-dfff7111be72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>word</th>\n",
       "      <th>hdt_lemma</th>\n",
       "      <th>pred_lemma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>Zudem würden bei der Portierung einer Rufnummer von einem Anbieter zu dem anderen einige Leistungsmerkmale womöglich nicht mehr zu der Verfügung stehen etwa bestimmte Mailboxdienste</td>\n",
       "      <td>anderen</td>\n",
       "      <td>anderer</td>\n",
       "      <td>ander</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>Zudem würden bei der Portierung einer Rufnummer von einem Anbieter zu dem anderen einige Leistungsmerkmale womöglich nicht mehr zu der Verfügung stehen etwa bestimmte Mailboxdienste</td>\n",
       "      <td>mehr</td>\n",
       "      <td>mehr</td>\n",
       "      <td>viel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>Die Betreiber von Mobilfunknetzen müssen der RegTP vierteljährlich über den Verlauf der Realisierungsarbeiten berichten</td>\n",
       "      <td>RegTP</td>\n",
       "      <td>RegTP</td>\n",
       "      <td>Regtp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>Das Unternehmen musste jetzt bei dem Potsdamer Amtsgericht Insolvenz anmelden</td>\n",
       "      <td>Potsdamer</td>\n",
       "      <td>Potsdamer</td>\n",
       "      <td>potsdamer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>Über die Gründe der Insolvenz konnte Unternehmenssprecher Johannes noch nichts sagen kündigte jedoch die Bekanntgabe weiterer Details für die Mitte dieser Woche an</td>\n",
       "      <td>weiterer</td>\n",
       "      <td>weit</td>\n",
       "      <td>weiter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324</th>\n",
       "      <td>Die RegTP erteilte die Genehmigung weil ihr die bisher vorliegenden Daten nicht ausreichend erschienen um das abschließend beurteilen zu können</td>\n",
       "      <td>Daten</td>\n",
       "      <td>Datum</td>\n",
       "      <td>Daten</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>338</th>\n",
       "      <td>Nutzer des können für Mark monatlich einschließlich und für normale Werktage an und Feiertagen kostenlos in dem gesamten deutschen telefonieren</td>\n",
       "      <td>können</td>\n",
       "      <td>können</td>\n",
       "      <td>Können</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>Seit dem heutigen Mittwochmorgen kursierten an der Frankfurter Börse Gerüchte denen zufolge Ron Sommer zurücktreten wird</td>\n",
       "      <td>Frankfurter</td>\n",
       "      <td>Frankfurter</td>\n",
       "      <td>frankfurter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>Letztlich dürfte es sich bei dem Gerücht lediglich um eine Begleiterscheinung der Talfahrt des Kurses der handeln</td>\n",
       "      <td>handeln</td>\n",
       "      <td>handeln</td>\n",
       "      <td>Handeln</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>431</th>\n",
       "      <td>Der Nemax50 verlor seit heute Morgen knapp fünf Prozent</td>\n",
       "      <td>Morgen</td>\n",
       "      <td>Morgen</td>\n",
       "      <td>morgen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449</th>\n",
       "      <td>Der Kölner Stadtnetzbetreiber NetCologne will mit Mannesmann Arcor einen Vertrag über den Zugang zu der Teilnehmeranschlussleitung der so genannten letzten Meile abschließen</td>\n",
       "      <td>Kölner</td>\n",
       "      <td>Kölner</td>\n",
       "      <td>kölner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>Weitere Details sollen erst an dem morgigen Donnerstag bekannt gegeben werden</td>\n",
       "      <td>Weitere</td>\n",
       "      <td>weit</td>\n",
       "      <td>weiter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>584</th>\n",
       "      <td>Entsprechende Spekulationen gab es bereits seit längerem</td>\n",
       "      <td>längerem</td>\n",
       "      <td>lang</td>\n",
       "      <td>lange</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>709</th>\n",
       "      <td>Die Bundesregierung spricht sich desweiteren dafür aus dass die Abgaben bei wegen der mehrfachen Überschreibbarkeit der Medien höher ausfallen sollten als bei da hier nur eine einmalige Vervielfältigungsmöglichkeit gegeben sei</td>\n",
       "      <td>Medien</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Media</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>816</th>\n",
       "      <td>Der Vergütungsbericht der Bundesregierung wird in Abständen in dem Auftrag des Bundestages erstellt um über die Entwicklung und Angemessenheit von Vergütungen nach 54 Urheberrechtsgesetz UrhG und insbesondere der Anlage zu 54 d 1 UrhG zu berichten</td>\n",
       "      <td>UrhG</td>\n",
       "      <td>UrhG</td>\n",
       "      <td>Urhg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>866</th>\n",
       "      <td>Geplant ist gleich eine ganze Serie von Spielen zu veröffentlichen die sich nach Aussage von Don Mattrick President der EA worldwide studios eng an den Romanvorlagen und an dem demnächst erscheinenden Harry orientieren</td>\n",
       "      <td>Spielen</td>\n",
       "      <td>Spiel</td>\n",
       "      <td>Spielen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1021</th>\n",
       "      <td>Grund für die Sperrung ist nach offiziellen Angaben dass auf diesen Seiten pornografisches Material und anderes Anstößiges zu finden sei</td>\n",
       "      <td>anderes</td>\n",
       "      <td>anderer</td>\n",
       "      <td>ander</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1052</th>\n",
       "      <td>Yahoo hat durch die Maßnahme jetzt in einem weiteren Land mit Problemen zu kämpfen</td>\n",
       "      <td>weiteren</td>\n",
       "      <td>weit</td>\n",
       "      <td>weiter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1116</th>\n",
       "      <td>Der Kopierschutz besteht nach Angaben von Intel aus einem das die sichere Verwaltung von Musik auf dem PC und deren Übertragung auf tragbare Audiogeräte und Speichermedien ermöglichen soll</td>\n",
       "      <td>PC</td>\n",
       "      <td>PC</td>\n",
       "      <td>Pc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1125</th>\n",
       "      <td>Intels Softwarepaket soll die entscheidenden Codes Schlüssel und andere wichtige Informationen der vor dem Zugriff Dritter verbergen und Versuche erkennen den Sicherheitsmechanismus zu überwinden</td>\n",
       "      <td>andere</td>\n",
       "      <td>anderer</td>\n",
       "      <td>ander</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1217</th>\n",
       "      <td>Der Messenger kommt für Linux in dem und umfasst gerade einmal knapp über 100 KByte</td>\n",
       "      <td>KByte</td>\n",
       "      <td>KByte</td>\n",
       "      <td>Kbyte</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1271</th>\n",
       "      <td>Die Bonner Staatsanwaltschaft nahm wegen des Verdachts auf Kapitalanlagebetrug Ermittlungen gegen mehrere Vorstände auf</td>\n",
       "      <td>Bonner</td>\n",
       "      <td>Bonner</td>\n",
       "      <td>bonner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1520</th>\n",
       "      <td>Nach dem März können sich für flat den Pauschaltarif für und nicht mehr neu anmelden</td>\n",
       "      <td>flat</td>\n",
       "      <td>flat</td>\n",
       "      <td>Flat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1626</th>\n",
       "      <td>Verbringt er in dem Monat weniger Stunden in dem Internet als er bezahlt hat so verfallen die restlichen Stunden surft er dagegen mehr kostet jede weitere Minute Pfennig</td>\n",
       "      <td>weitere</td>\n",
       "      <td>weit</td>\n",
       "      <td>weiter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1671</th>\n",
       "      <td>Tiefrot waren die Zahlen die die Ende Januar auf der Bilanzpressekonferenz zu verkünden hatte</td>\n",
       "      <td>Tiefrot</td>\n",
       "      <td>Tiefrot</td>\n",
       "      <td>tiefrot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1805</th>\n",
       "      <td>Nicht die sind nach seinen Worten die größten Barrieren für den neuer Zielgruppen sondern die unverändert hohen</td>\n",
       "      <td>hohen</td>\n",
       "      <td>hoch</td>\n",
       "      <td>hoh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1871</th>\n",
       "      <td>Callahan übernimmt 55 Prozent der Anteile von der Deutschen Telekom und will das Kabelnetz für interaktives Fernsehen und einen schnellen ausbauen</td>\n",
       "      <td>ausbauen</td>\n",
       "      <td>ausbauen</td>\n",
       "      <td>Ausbau</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2271</th>\n",
       "      <td>Die die als 347 MByte großes gepacktes auf die Platte kommt enthält neben dem eigentlichen Linux noch den XMMS den Acrobat Reader den Macromedia Flash Player 4 einen Instant Messenger die eFax sowie den Netscape Communicator</td>\n",
       "      <td>MByte</td>\n",
       "      <td>MByte</td>\n",
       "      <td>Mbyte</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2283</th>\n",
       "      <td>Die die als 347 MByte großes gepacktes auf die Platte kommt enthält neben dem eigentlichen Linux noch den XMMS den Acrobat Reader den Macromedia Flash Player 4 einen Instant Messenger die eFax sowie den Netscape Communicator</td>\n",
       "      <td>eFax</td>\n",
       "      <td>eFax</td>\n",
       "      <td>Efax</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2285</th>\n",
       "      <td>Die die für knapp 25 auf CD bestellt werden kann bietet zusätzlich einen User Guide technischen Support per für 30 Tage sowie das Loki Entertainment Pack mit einigen</td>\n",
       "      <td>CD</td>\n",
       "      <td>CD</td>\n",
       "      <td>Cd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2419</th>\n",
       "      <td>Insgesamt arbeiten momentan noch rund 250 Mitarbeiter in der Dortmunder Niederlassung</td>\n",
       "      <td>Dortmunder</td>\n",
       "      <td>Dortmunder</td>\n",
       "      <td>dortmunder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2421</th>\n",
       "      <td>Zukünftig will Versatel enger mit dem Tochterunternehmen KomTel in Flensburg zusammenarbeiten und nutzen</td>\n",
       "      <td>Zukünftig</td>\n",
       "      <td>Zukünftig</td>\n",
       "      <td>zukünftig</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2558</th>\n",
       "      <td>Mit dem türkischsprachigen Angebot steht aber nicht alleine dar</td>\n",
       "      <td>alleine</td>\n",
       "      <td>alleine</td>\n",
       "      <td>allein</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2722</th>\n",
       "      <td>Wenn da jetzt 5 Prozent Kunden anderer Provider hinzukämen würde das kaum etwas ausmachen</td>\n",
       "      <td>anderer</td>\n",
       "      <td>anderer</td>\n",
       "      <td>ander</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2738</th>\n",
       "      <td>Wenn es also tatsächlich zu Engpässen kommen würde dann sei die Telekom schuld und niemand anderes</td>\n",
       "      <td>schuld</td>\n",
       "      <td>schuld</td>\n",
       "      <td>Schuld</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2780</th>\n",
       "      <td>Wenn der VATM jetzt argumentiere dass die Provider zu hohe Kosten aufzuwenden hätten um eine bundesweite Flatrate anbieten zu können dann sei das so nicht richtig</td>\n",
       "      <td>hohe</td>\n",
       "      <td>hoch</td>\n",
       "      <td>hoh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3263</th>\n",
       "      <td>Dass die Entscheidung des Gerichtes auch auf die Einstellung der zurückzuführen ist findet AOL bezeichnend</td>\n",
       "      <td>zurückzuführen</td>\n",
       "      <td>zurückführen</td>\n",
       "      <td>zurückzuführ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3838</th>\n",
       "      <td>Wie Ministeriumssprecher Michael Knaps gegenüber heise online sagte wollte die Mordkommission ein Forum auf einer Alfelder zu der Suche nach Hinweisen in einem Mordfall nutzen</td>\n",
       "      <td>Alfelder</td>\n",
       "      <td>Alfelder</td>\n",
       "      <td>alfeld</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3921</th>\n",
       "      <td>Dass Streamgate zu seinem Vorteil den Unmut all jener ausnutzen wollte die derzeit immer noch zu Hunderttausenden auf den bestellten warten und sich zudem durch die penetrante Robert Werbung veralbert fühlen dürften wird der Telekom nicht gefallen haben</td>\n",
       "      <td>warten</td>\n",
       "      <td>warten</td>\n",
       "      <td>Warte</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3977</th>\n",
       "      <td>Enthalten ist darin ein Transfervolumen von 500 MByte jedes weitere GByte kostet 69 Mark</td>\n",
       "      <td>GByte</td>\n",
       "      <td>GByte</td>\n",
       "      <td>Gbyte</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4046</th>\n",
       "      <td>Es sollen keine festen vergeben werden bei Inaktivität wird die Verbindung getrennt</td>\n",
       "      <td>festen</td>\n",
       "      <td>fest</td>\n",
       "      <td>Feste</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4149</th>\n",
       "      <td>Vorher waren bereits die Star Telecom und TelDaFax davon betroffen</td>\n",
       "      <td>betroffen</td>\n",
       "      <td>betreffen</td>\n",
       "      <td>betroffen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4347</th>\n",
       "      <td>Betroffen waren laut Clemens Gerth zirka Kunden die ihre über den Berliner Anbieter laufen lassen</td>\n",
       "      <td>Berliner</td>\n",
       "      <td>Berliner</td>\n",
       "      <td>berliner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4415</th>\n",
       "      <td>Ein soll das Auslesen von und die Umwandlung der Songs in das ermöglichen</td>\n",
       "      <td>ermöglichen</td>\n",
       "      <td>ermöglichen</td>\n",
       "      <td>Ermöglicher</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4427</th>\n",
       "      <td>Wer damit eine auslesen und auf die Festplatte kopieren wollte war auf das WMA angewiesen</td>\n",
       "      <td>angewiesen</td>\n",
       "      <td>anweisen</td>\n",
       "      <td>angewiesen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4434</th>\n",
       "      <td>Ein weiteres soll das Abspielen von DVDs ermöglichen</td>\n",
       "      <td>weiteres</td>\n",
       "      <td>weit</td>\n",
       "      <td>weiter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4446</th>\n",
       "      <td>Das MP3 Creation Pack und das DVD Decoder Pack kann man unabhängig voneinander kaufen</td>\n",
       "      <td>MP3</td>\n",
       "      <td>MP3</td>\n",
       "      <td>mp3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4447</th>\n",
       "      <td>Das MP3 Creation Pack und das DVD Decoder Pack kann man unabhängig voneinander kaufen</td>\n",
       "      <td>DVD</td>\n",
       "      <td>DVD</td>\n",
       "      <td>Dvd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4468</th>\n",
       "      <td>Hierzu hatte das Unternehmen Anfang Mai die Voraussetzungen geschaffen und mit einen Vertrag über die Anmietung von geschlossen</td>\n",
       "      <td>geschlossen</td>\n",
       "      <td>schließen</td>\n",
       "      <td>geschlossen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4497</th>\n",
       "      <td>Schuld an der Verschiebung ist laut Kiyoyuki Tsujimura Managing Director von DoCoMo nicht das eigene Unternehmen Schuld seien die europäischen Netzanbieter</td>\n",
       "      <td>DoCoMo</td>\n",
       "      <td>DoCoMo</td>\n",
       "      <td>Docomo</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                           sentence  \\\n",
       "156                                                                           Zudem würden bei der Portierung einer Rufnummer von einem Anbieter zu dem anderen einige Leistungsmerkmale womöglich nicht mehr zu der Verfügung stehen etwa bestimmte Mailboxdienste   \n",
       "159                                                                           Zudem würden bei der Portierung einer Rufnummer von einem Anbieter zu dem anderen einige Leistungsmerkmale womöglich nicht mehr zu der Verfügung stehen etwa bestimmte Mailboxdienste   \n",
       "195                                                                                                                                         Die Betreiber von Mobilfunknetzen müssen der RegTP vierteljährlich über den Verlauf der Realisierungsarbeiten berichten   \n",
       "263                                                                                                                                                                                   Das Unternehmen musste jetzt bei dem Potsdamer Amtsgericht Insolvenz anmelden   \n",
       "280                                                                                             Über die Gründe der Insolvenz konnte Unternehmenssprecher Johannes noch nichts sagen kündigte jedoch die Bekanntgabe weiterer Details für die Mitte dieser Woche an   \n",
       "324                                                                                                                 Die RegTP erteilte die Genehmigung weil ihr die bisher vorliegenden Daten nicht ausreichend erschienen um das abschließend beurteilen zu können   \n",
       "338                                                                                                                 Nutzer des können für Mark monatlich einschließlich und für normale Werktage an und Feiertagen kostenlos in dem gesamten deutschen telefonieren   \n",
       "365                                                                                                                                        Seit dem heutigen Mittwochmorgen kursierten an der Frankfurter Börse Gerüchte denen zufolge Ron Sommer zurücktreten wird   \n",
       "392                                                                                                                                               Letztlich dürfte es sich bei dem Gerücht lediglich um eine Begleiterscheinung der Talfahrt des Kurses der handeln   \n",
       "431                                                                                                                                                                                                         Der Nemax50 verlor seit heute Morgen knapp fünf Prozent   \n",
       "449                                                                                   Der Kölner Stadtnetzbetreiber NetCologne will mit Mannesmann Arcor einen Vertrag über den Zugang zu der Teilnehmeranschlussleitung der so genannten letzten Meile abschließen   \n",
       "475                                                                                                                                                                                   Weitere Details sollen erst an dem morgigen Donnerstag bekannt gegeben werden   \n",
       "584                                                                                                                                                                                                        Entsprechende Spekulationen gab es bereits seit längerem   \n",
       "709                              Die Bundesregierung spricht sich desweiteren dafür aus dass die Abgaben bei wegen der mehrfachen Überschreibbarkeit der Medien höher ausfallen sollten als bei da hier nur eine einmalige Vervielfältigungsmöglichkeit gegeben sei   \n",
       "816         Der Vergütungsbericht der Bundesregierung wird in Abständen in dem Auftrag des Bundestages erstellt um über die Entwicklung und Angemessenheit von Vergütungen nach 54 Urheberrechtsgesetz UrhG und insbesondere der Anlage zu 54 d 1 UrhG zu berichten   \n",
       "866                                      Geplant ist gleich eine ganze Serie von Spielen zu veröffentlichen die sich nach Aussage von Don Mattrick President der EA worldwide studios eng an den Romanvorlagen und an dem demnächst erscheinenden Harry orientieren   \n",
       "1021                                                                                                                       Grund für die Sperrung ist nach offiziellen Angaben dass auf diesen Seiten pornografisches Material und anderes Anstößiges zu finden sei   \n",
       "1052                                                                                                                                                                             Yahoo hat durch die Maßnahme jetzt in einem weiteren Land mit Problemen zu kämpfen   \n",
       "1116                                                                   Der Kopierschutz besteht nach Angaben von Intel aus einem das die sichere Verwaltung von Musik auf dem PC und deren Übertragung auf tragbare Audiogeräte und Speichermedien ermöglichen soll   \n",
       "1125                                                            Intels Softwarepaket soll die entscheidenden Codes Schlüssel und andere wichtige Informationen der vor dem Zugriff Dritter verbergen und Versuche erkennen den Sicherheitsmechanismus zu überwinden   \n",
       "1217                                                                                                                                                                            Der Messenger kommt für Linux in dem und umfasst gerade einmal knapp über 100 KByte   \n",
       "1271                                                                                                                                        Die Bonner Staatsanwaltschaft nahm wegen des Verdachts auf Kapitalanlagebetrug Ermittlungen gegen mehrere Vorstände auf   \n",
       "1520                                                                                                                                                                           Nach dem März können sich für flat den Pauschaltarif für und nicht mehr neu anmelden   \n",
       "1626                                                                                      Verbringt er in dem Monat weniger Stunden in dem Internet als er bezahlt hat so verfallen die restlichen Stunden surft er dagegen mehr kostet jede weitere Minute Pfennig   \n",
       "1671                                                                                                                                                                  Tiefrot waren die Zahlen die die Ende Januar auf der Bilanzpressekonferenz zu verkünden hatte   \n",
       "1805                                                                                                                                                Nicht die sind nach seinen Worten die größten Barrieren für den neuer Zielgruppen sondern die unverändert hohen   \n",
       "1871                                                                                                             Callahan übernimmt 55 Prozent der Anteile von der Deutschen Telekom und will das Kabelnetz für interaktives Fernsehen und einen schnellen ausbauen   \n",
       "2271                               Die die als 347 MByte großes gepacktes auf die Platte kommt enthält neben dem eigentlichen Linux noch den XMMS den Acrobat Reader den Macromedia Flash Player 4 einen Instant Messenger die eFax sowie den Netscape Communicator   \n",
       "2283                               Die die als 347 MByte großes gepacktes auf die Platte kommt enthält neben dem eigentlichen Linux noch den XMMS den Acrobat Reader den Macromedia Flash Player 4 einen Instant Messenger die eFax sowie den Netscape Communicator   \n",
       "2285                                                                                          Die die für knapp 25 auf CD bestellt werden kann bietet zusätzlich einen User Guide technischen Support per für 30 Tage sowie das Loki Entertainment Pack mit einigen   \n",
       "2419                                                                                                                                                                          Insgesamt arbeiten momentan noch rund 250 Mitarbeiter in der Dortmunder Niederlassung   \n",
       "2421                                                                                                                                                       Zukünftig will Versatel enger mit dem Tochterunternehmen KomTel in Flensburg zusammenarbeiten und nutzen   \n",
       "2558                                                                                                                                                                                                Mit dem türkischsprachigen Angebot steht aber nicht alleine dar   \n",
       "2722                                                                                                                                                                      Wenn da jetzt 5 Prozent Kunden anderer Provider hinzukämen würde das kaum etwas ausmachen   \n",
       "2738                                                                                                                                                             Wenn es also tatsächlich zu Engpässen kommen würde dann sei die Telekom schuld und niemand anderes   \n",
       "2780                                                                                             Wenn der VATM jetzt argumentiere dass die Provider zu hohe Kosten aufzuwenden hätten um eine bundesweite Flatrate anbieten zu können dann sei das so nicht richtig   \n",
       "3263                                                                                                                                                     Dass die Entscheidung des Gerichtes auch auf die Einstellung der zurückzuführen ist findet AOL bezeichnend   \n",
       "3838                                                                                Wie Ministeriumssprecher Michael Knaps gegenüber heise online sagte wollte die Mordkommission ein Forum auf einer Alfelder zu der Suche nach Hinweisen in einem Mordfall nutzen   \n",
       "3921  Dass Streamgate zu seinem Vorteil den Unmut all jener ausnutzen wollte die derzeit immer noch zu Hunderttausenden auf den bestellten warten und sich zudem durch die penetrante Robert Werbung veralbert fühlen dürften wird der Telekom nicht gefallen haben   \n",
       "3977                                                                                                                                                                       Enthalten ist darin ein Transfervolumen von 500 MByte jedes weitere GByte kostet 69 Mark   \n",
       "4046                                                                                                                                                                            Es sollen keine festen vergeben werden bei Inaktivität wird die Verbindung getrennt   \n",
       "4149                                                                                                                                                                                             Vorher waren bereits die Star Telecom und TelDaFax davon betroffen   \n",
       "4347                                                                                                                                                              Betroffen waren laut Clemens Gerth zirka Kunden die ihre über den Berliner Anbieter laufen lassen   \n",
       "4415                                                                                                                                                                                      Ein soll das Auslesen von und die Umwandlung der Songs in das ermöglichen   \n",
       "4427                                                                                                                                                                      Wer damit eine auslesen und auf die Festplatte kopieren wollte war auf das WMA angewiesen   \n",
       "4434                                                                                                                                                                                                           Ein weiteres soll das Abspielen von DVDs ermöglichen   \n",
       "4446                                                                                                                                                                          Das MP3 Creation Pack und das DVD Decoder Pack kann man unabhängig voneinander kaufen   \n",
       "4447                                                                                                                                                                          Das MP3 Creation Pack und das DVD Decoder Pack kann man unabhängig voneinander kaufen   \n",
       "4468                                                                                                                                Hierzu hatte das Unternehmen Anfang Mai die Voraussetzungen geschaffen und mit einen Vertrag über die Anmietung von geschlossen   \n",
       "4497                                                                                                    Schuld an der Verschiebung ist laut Kiyoyuki Tsujimura Managing Director von DoCoMo nicht das eigene Unternehmen Schuld seien die europäischen Netzanbieter   \n",
       "\n",
       "                word     hdt_lemma    pred_lemma  \n",
       "156          anderen       anderer         ander  \n",
       "159             mehr          mehr          viel  \n",
       "195            RegTP         RegTP         Regtp  \n",
       "263        Potsdamer     Potsdamer     potsdamer  \n",
       "280         weiterer          weit        weiter  \n",
       "324            Daten         Datum         Daten  \n",
       "338           können        können        Können  \n",
       "365      Frankfurter   Frankfurter   frankfurter  \n",
       "392          handeln       handeln       Handeln  \n",
       "431           Morgen        Morgen        morgen  \n",
       "449           Kölner        Kölner        kölner  \n",
       "475          Weitere          weit        weiter  \n",
       "584         längerem          lang         lange  \n",
       "709           Medien        Medium         Media  \n",
       "816             UrhG          UrhG          Urhg  \n",
       "866          Spielen         Spiel       Spielen  \n",
       "1021         anderes       anderer         ander  \n",
       "1052        weiteren          weit        weiter  \n",
       "1116              PC            PC            Pc  \n",
       "1125          andere       anderer         ander  \n",
       "1217           KByte         KByte         Kbyte  \n",
       "1271          Bonner        Bonner        bonner  \n",
       "1520            flat          flat          Flat  \n",
       "1626         weitere          weit        weiter  \n",
       "1671         Tiefrot       Tiefrot       tiefrot  \n",
       "1805           hohen          hoch           hoh  \n",
       "1871        ausbauen      ausbauen        Ausbau  \n",
       "2271           MByte         MByte         Mbyte  \n",
       "2283            eFax          eFax          Efax  \n",
       "2285              CD            CD            Cd  \n",
       "2419      Dortmunder    Dortmunder    dortmunder  \n",
       "2421       Zukünftig     Zukünftig     zukünftig  \n",
       "2558         alleine       alleine        allein  \n",
       "2722         anderer       anderer         ander  \n",
       "2738          schuld        schuld        Schuld  \n",
       "2780            hohe          hoch           hoh  \n",
       "3263  zurückzuführen  zurückführen  zurückzuführ  \n",
       "3838        Alfelder      Alfelder        alfeld  \n",
       "3921          warten        warten         Warte  \n",
       "3977           GByte         GByte         Gbyte  \n",
       "4046          festen          fest         Feste  \n",
       "4149       betroffen     betreffen     betroffen  \n",
       "4347        Berliner      Berliner      berliner  \n",
       "4415     ermöglichen   ermöglichen   Ermöglicher  \n",
       "4427      angewiesen      anweisen    angewiesen  \n",
       "4434        weiteres          weit        weiter  \n",
       "4446             MP3           MP3           mp3  \n",
       "4447             DVD           DVD           Dvd  \n",
       "4468     geschlossen     schließen   geschlossen  \n",
       "4497          DoCoMo        DoCoMo        Docomo  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fails_df = hdt_res[(hdt_res.hdt_lemma!=hdt_res.pred_lemma)&(hdt_res.hdt_lemma==hdt_res.spacy_lemma)].drop_duplicates(subset=['word','hdt_lemma'])\n",
    "fails_df.loc[~fails_df.pred_lemma.isna(),['sentence','word','hdt_lemma','pred_lemma']].head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "216babeb-0337-439d-8038-4843a0a710c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glemma.glemma import GLemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e72e6d-331f-4648-8493-5560bcb7c339",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = GLemma(\n",
    "                    wordfreq_csv='../glemma/data/third-party/FrequencyWords/content/2018/de/de_full.txt',\n",
    "                    use_nouns_nbc=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
