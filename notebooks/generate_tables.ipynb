{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da6f45f6-3a1d-4659-9ae1-9c08295a7754",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import re\n",
    "from collections import defaultdict,namedtuple\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('max_colwidth', 400)\n",
    "\n",
    "parser_dir = '/lustre/groups/epigenereg01/workspace/projects/vale/bio/c2/resources/tools/wiktionary-de-parser'\n",
    "sys.path.append(parser_dir)\n",
    "\n",
    "from wiktionary_de_parser.dump_processor import WiktionaryDump\n",
    "from wiktionary_de_parser import WiktionaryParser\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23f9a5e8-c41f-484a-964a-7c363bcad6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_german(wiki_record):\n",
    "    return wiki_record.language.lang == 'Deutsch'\n",
    "\n",
    "def get_flexion_field(wiki_record, field_name):\n",
    "    if field_name in wiki_record.flexion:\n",
    "        wordform = wiki_record.flexion[field_name].strip()\n",
    "        wordform = re.sub(r\".*:'' \",r\"\",wordform) #remarks like 'selten:'/'militarisch:'\n",
    "        return wordform\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf9e315b-9286-4766-85b4-b75c65344024",
   "metadata": {},
   "outputs": [],
   "source": [
    "dump = WiktionaryDump(\n",
    "    dump_file_path=parser_dir + \"/wiktionary_german/dewiktionary-latest-pages-articles-multistream.xml.bz2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4e1461-38b2-4659-9113-cdda1768ed90",
   "metadata": {},
   "source": [
    "# Generate nouns lookup table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6e64e8cb-8885-4d3e-a3e0-bb517c3de7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_noun(wiki_record):\n",
    "    return (wiki_record.pos \n",
    "            and 'Substantiv' in wiki_record.pos \n",
    "            and wiki_record.flexion is not None #we can't do much without any flexion information\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c38c997e-1a40-48c0-b371-b693bb0770e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_noun_wordforms_adjective_declination(lemma):\n",
    "    '''\n",
    "    Decline nouns as adjectives\n",
    "    e.g. Beamte(r), Unbekannte(r)\n",
    "    '''\n",
    "\n",
    "    strong_declinations = {'m':{'Nominativ Singular':'er','Genitiv Singular':'en','Dativ Singular':'em','Akkusativ Singular':'en',\n",
    "                           'Nominativ Plural':'e', 'Genitiv Plural':'er','Dativ Plural':'en', 'Akkusativ Plural':'e'},\n",
    "                           'f':{'Nominativ Singular':'e','Genitiv Singular':'er','Dativ Singular':'er','Akkusativ Singular':'e',\n",
    "                           'Nominativ Plural':'e', 'Genitiv Plural':'er','Dativ Plural':'en', 'Akkusativ Plural':'e'},\n",
    "                           'n':{'Nominativ Singular':'es','Genitiv Singular':'en','Dativ Singular':'em','Akkusativ Singular':'es',\n",
    "                           'Nominativ Plural':'e', 'Genitiv Plural':'er','Dativ Plural':'en', 'Akkusativ Plural':'e'},}\n",
    "    \n",
    "    weak_declinations = {'m':{'Nominativ Singular':'e','Genitiv Singular':'en','Dativ Singular':'en','Akkusativ Singular':'en',\n",
    "                           'Nominativ Plural':'en', 'Genitiv Plural':'en','Dativ Plural':'en', 'Akkusativ Plural':'en'},\n",
    "                           'f':{'Nominativ Singular':'e','Genitiv Singular':'en','Dativ Singular':'en','Akkusativ Singular':'e',\n",
    "                           'Nominativ Plural':'en', 'Genitiv Plural':'en','Dativ Plural':'en', 'Akkusativ Plural':'en'},\n",
    "                           'n':{'Nominativ Singular':'e','Genitiv Singular':'en','Dativ Singular':'en','Akkusativ Singular':'e',\n",
    "                           'Nominativ Plural':'en', 'Genitiv Plural':'en','Dativ Plural':'en', 'Akkusativ Plural':'en'},}\n",
    "    \n",
    "    mixed_declinations = {'m':{'Nominativ Singular':'er','Genitiv Singular':'en','Dativ Singular':'en','Akkusativ Singular':'en',\n",
    "                           'Nominativ Plural':'en', 'Genitiv Plural':'en','Dativ Plural':'en', 'Akkusativ Plural':'en'},\n",
    "                           'f':{'Nominativ Singular':'e','Genitiv Singular':'en','Dativ Singular':'en','Akkusativ Singular':'e',\n",
    "                           'Nominativ Plural':'en', 'Genitiv Plural':'en','Dativ Plural':'en', 'Akkusativ Plural':'en'},\n",
    "                           'n':{'Nominativ Singular':'es','Genitiv Singular':'en','Dativ Singular':'en','Akkusativ Singular':'es',\n",
    "                           'Nominativ Plural':'en', 'Genitiv Plural':'en','Dativ Plural':'en', 'Akkusativ Plural':'en'},}\n",
    "    \n",
    "    wordforms = defaultdict(set)\n",
    "        \n",
    "    Lemma = namedtuple('lemma', 'lemma declination genus')\n",
    "    \n",
    "    for declinations in (strong_declinations,weak_declinations,mixed_declinations):\n",
    "        for genus in ('m','f','n'):\n",
    "            for category, ending in declinations[genus].items():\n",
    "                wordform = re.sub(r'er?$','',lemma) + ending\n",
    "                wordforms[wordform].add(Lemma(lemma,category,genus))\n",
    "\n",
    "    return wordforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c0de3be6-1705-4ad1-b0d5-3aca06bd19ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_noun_forms(wiki_record):\n",
    "\n",
    "    base_categories = ['Nominativ Singular', 'Nominativ Plural', 'Genitiv Singular', 'Genitiv Plural', 'Dativ Singular', \n",
    "                         'Dativ Plural', 'Akkusativ Singular', 'Akkusativ Plural']\n",
    "    \n",
    "    #remove these rare nouns in order not to confuse with some forms of more common nouns\n",
    "    stoplist = {'gedanken':'.*', 'real':'.*', 'studium':'Plural', 'fleck':'Plural 2', 'post':'Plural', 'gemein':'.*', 'willen':'.*',\n",
    "                'namen':'.*', 'arme':'.*', 'schade':'Plural', 'zeug':'Plural', 'omme':'.*', 'praxis':'Plural 2','schranken':'.*','fliegen':'.*'}\n",
    "\n",
    "    Lemma = namedtuple('lemma', 'lemma declination genus')\n",
    "\n",
    "    lemmas = defaultdict(set)\n",
    "\n",
    "    lemma = wiki_record.lemma.lemma.lower() #we will use lowercase lemmas in the dictionary\n",
    "\n",
    "    if ' ' in lemma:\n",
    "        #we exclude fixed expressions consisting of multiple words (often entities, e.g. 'Vereinigte Arabische Emirate')\n",
    "        return {}\n",
    "\n",
    "    if 'adjektivische Deklination' in wiki_record.pos['Substantiv']:\n",
    "        #decline as an adjective\n",
    "        if lemma.endswith('r'):\n",
    "            #don't use feminine (use masculine) lemma for plural forms, in accordance with other lemmatizers\n",
    "            lemmas = get_noun_wordforms_adjective_declination(lemma)\n",
    "        \n",
    "    genus_categories = [x for x in wiki_record.flexion.keys() if x.startswith('Genus')] # 'Genus', 'Genus 1', 'Genus 2' in case the noun admits multiple genera\n",
    "            \n",
    "    if len(genus_categories)==0 and 'Nominativ Plural' in wiki_record.flexion:\n",
    "        #the noun admits only plural form\n",
    "        genus_categories = ['only_plural']\n",
    "\n",
    "    for genus_category in genus_categories:\n",
    "\n",
    "        if genus_category=='only_plural':\n",
    "            genus = 'only_plural'\n",
    "        else:\n",
    "            genus = wiki_record.flexion[genus_category]\n",
    "                        \n",
    "        for base_category in base_categories:\n",
    "\n",
    "            if len(genus_categories)==1:\n",
    "                #if there is only a single genus, extra forms ending with digits or stars are possible\n",
    "                extended_categories = [base_category+genus_suffix+'*'*n for genus_suffix in ('',' 1',' 2',' 3',' 4') for n in range(3)] # additional forms with an asterisk\n",
    "            else:\n",
    "                #if there are multiple genera, extra forms endings should match the genus index or be empty\n",
    "                genus_suffix = genus_category.replace('Genus','') #empty or ' 1', ' 2', etc...\n",
    "                extended_categories = [base_category+genus_suffix_+'*'*n for genus_suffix_ in ('',genus_suffix) for n in range(3)] # additional forms with an asterisk\n",
    "\n",
    "            for extended_category in extended_categories:\n",
    "\n",
    "                if lemma in stoplist and re.search(stoplist[lemma],extended_category):\n",
    "                    continue\n",
    "                        \n",
    "                if extended_category in wiki_record.flexion:\n",
    "                    \n",
    "                    wordform = get_flexion_field(wiki_record,extended_category).lower()\n",
    "        \n",
    "                    if not re.match('^[\\w -]+$',wordform):\n",
    "                        print(f'Unrecognized characters in wordform {wordform} for {lemma}')\n",
    "                        return {}\n",
    "          \n",
    "                    lemmas[wordform].add(Lemma(lemma,base_category,genus))#assign lemma to the wordform\n",
    "\n",
    "\n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "beb5a53b-d781-4171-8ddc-60276399bab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nouns(dump):\n",
    "\n",
    "    nouns = defaultdict(set)\n",
    "    \n",
    "    n_records = 0\n",
    "\n",
    "    parser = WiktionaryParser()\n",
    "\n",
    "    for page in dump.pages():\n",
    "        if page.redirect_to:\n",
    "            continue\n",
    "                \n",
    "        for entry in parser.entries_from_page(page):\n",
    "            wiki_record = parser.parse_entry(entry)\n",
    "            if is_german(wiki_record) and is_noun(wiki_record):\n",
    "                wordlemmas = get_noun_forms(wiki_record)\n",
    "                for k,v in wordlemmas.items():\n",
    "                    nouns[k] = nouns[k].union(v)\n",
    "                n_records += 1\n",
    "                if n_records%1000==0:\n",
    "                    print(f'{n_records} records processed')\n",
    "\n",
    "\n",
    "    nouns = {k:[lemma_record._asdict() for lemma_record in v] for k,v in nouns.items()} #defaultdict with sets of named tuples to dict with lists of dicts\n",
    "\n",
    "    return nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4501953f-1ac0-4383-be93-24a719882126",
   "metadata": {},
   "outputs": [],
   "source": [
    "nouns = get_nouns(dump)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41424b2d-bc65-4c03-9a2d-40389e3bb771",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for wordform, lemmas in nouns.items():\n",
    "#    lemmas_NP = []\n",
    "#    for lemma in lemmas:\n",
    "#        if lemma['declination'] == 'Nominativ Plural':\n",
    "#            lemmas_NP.append((lemma['lemma'],lemma['genus']))\n",
    "#    if lemmas_NP:\n",
    "#        lemmas_, genera_ = zip(*lemmas_NP)\n",
    "#        if 'only_plural' in genera_ and len(set(lemmas_))>1:\n",
    "#            nouns[wordform] = [lemma for lemma in nouns[wordform] if lemma['genus']=='only_plural' or (lemma['lemma'],lemma['genus']) not in lemmas_NP]\n",
    "#            print(wordform,lemmas_NP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "95e29f60-4dea-4bac-98cb-7482f1d3e97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('glemma/datanouns.json', 'wt', encoding='UTF-8') as json_file:\n",
    "#    json.dump(nouns, json_file, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d006ca14-569a-403a-b05b-717a0f89ab12",
   "metadata": {},
   "source": [
    "# Generate verb lookup table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2da436-7e8c-40f3-83f6-c8c48b5b275d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_verb(wiki_record):\n",
    "    return (wiki_record.pos \n",
    "            and 'Verb' in wiki_record.pos \n",
    "            and wiki_record.pos['Verb'] in ([],['Hilfsverb'])\n",
    "            and wiki_record.flexion is not None #we can't do much without any flexion information\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9c5333b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_verb(verb):\n",
    "    root = re.sub(r'e?n$|e$|e?t$|est$|([^s])st$',r'\\1', verb) #remove ending, leave s when it's part of the root, e.g. lassen->lass \n",
    "    return root\n",
    "\n",
    "def generate_verbforms(verb, lemma, category):\n",
    "    '''\n",
    "    Add verb forms that are usually absent in the Wiktionary flexion entry\n",
    "    '''\n",
    "    wordforms = [verb,]\n",
    "    \n",
    "    if category.startswith('Präsens_ich'):\n",
    "\n",
    "        if lemma == 'sein':\n",
    "\n",
    "            wordforms += ['sei','seiest','seist','seiet', 'seien', 'sind'] #Konjunktiv I, Indikativ 3. Person Plural\n",
    "\n",
    "        elif lemma in ('können', 'sollen', 'müssen', 'dürfen', 'wollen', 'mögen'):\n",
    "\n",
    "            wordforms += [lemma[:-1],lemma[:-1]+'st',lemma[:-1]+'t', lemma[:-2]+'t']\n",
    "            \n",
    "        elif lemma.endswith('ern') or lemma.endswith('eln'): \n",
    "            #wandern, sammeln\n",
    "            if verb.endswith('ere') or verb.endswith('ele'):\n",
    "                wordforms += [verb[:-1]+'st', verb[:-1]+'t']  #Indikativ 2. Person Plural, Konjunktiv I 2. Person Singular, Konjunktiv I 2. Person Plural\n",
    "            elif verb.endswith('le'):\n",
    "                wordforms += [verb+'st', verb+'t']  #Konjunktiv I 2. Person Singular, Konjunktiv I 2. Person Plural, alternative forms\n",
    "        else:\n",
    "\n",
    "            wordforms += [verb+'st', verb+'t']  #Konjunktiv I 2. Person Singular, Konjunktiv I 2. Person Plural\n",
    "                \n",
    "            verb = re.sub(r'([^td])e$|([wtzpsdfghjkxvb][mn]e)$',r'\\1\\2',verb) #don't remove e if preceeded by t or d or m,n after a consonant (except l,r,l,m)\n",
    "                \n",
    "            wordforms += [verb+'st', verb+'t']  #Indikativ 2. Person Plural\n",
    "\n",
    "        if lemma.endswith('auern'):\n",
    "            #bedauern, kauern\n",
    "            wordforms += [re.sub(r'auere$',r'aure',verb)] #bedauere-->bedaure\n",
    "\n",
    "        if lemma == 'werden':\n",
    "            wordforms += ['worden']\n",
    "           \n",
    "    elif category.startswith('Präteritum_ich'):\n",
    "        \n",
    "        if verb[-1]=='e':\n",
    "            #regular + mixed verbs\n",
    "            #machen, denken\n",
    "            wordforms += [verb+'st', verb+'t', verb+'n']\n",
    "        elif verb[-1] in ('s','ß','z'):\n",
    "            #lassen,schmelzen,blasen\n",
    "            wordforms += [verb+'est', verb+'t', verb+'en']  \n",
    "        elif re.search(r'([td]$)|([wtzpsdfghjkxvb][mn]$)', verb):\n",
    "            #halten, finden\n",
    "            wordforms += [verb+'est', verb+'st', verb+'et', verb+'en']  \n",
    "        else:\n",
    "            #irregular verbs\n",
    "            #sprechen\n",
    "            wordforms += [verb+'st', verb+'t', verb+'en']  \n",
    "\n",
    "    elif category.startswith('Konjunktiv II_ich'):\n",
    "\n",
    "        if lemma=='fahren':\n",
    "            #don't add Konjunktiv II for 'fahren' to avoid confusuion with Indikativ for 'führen'\n",
    "            return []\n",
    "            \n",
    "        wordforms += [verb+'st', verb+'t', verb+'n'] #Konjunktiv II 2. Person Singular, Konjunktiv II 2.,3. Person Plural\n",
    "            \n",
    "    return wordforms\n",
    "    \n",
    "def get_verb_forms(wiki_record):\n",
    "\n",
    "    base_categories = ['Präsens_ich', 'Präsens_du', 'Präsens_er, sie, es', 'Präteritum_ich', 'Konjunktiv II_ich', \n",
    "                         'Imperativ Singular', 'Imperativ Plural']\n",
    "    \n",
    "    categories = [x+'*'*n for x in base_categories for n in range(4)] + ['Imperativ Singular 2']\n",
    "\n",
    "    Lemma = namedtuple('lemma', 'lemma connection via')\n",
    "\n",
    "    verblemmas = defaultdict(set)\n",
    "\n",
    "    lemma = wiki_record.lemma.lemma #word lemma\n",
    "    \n",
    "    if ' ' in lemma:\n",
    "        #we ignore the cases where the prefix is not attached to the verb in subordinate clauses\n",
    "        #e.g. frei geben, bekannt machen\n",
    "        #dependency parses like SpaCy can't recognize that these are both parts of the same verb anyway\n",
    "        return {}, None\n",
    "\n",
    "    verblemmas[lemma].add(Lemma(lemma,None,()))\n",
    "\n",
    "    is_separable, prefix = False, None\n",
    "\n",
    "    for category in categories:\n",
    "        \n",
    "        #category with asterisk for alternative forms, e.g. ich anerkenne, ich erkenne an\n",
    "        \n",
    "        if category in wiki_record.flexion:\n",
    "            \n",
    "            verb = get_flexion_field(wiki_record,category)\n",
    "\n",
    "            if not re.match('^[\\w ]+$',verb):\n",
    "                print(f'Unrecognized characters in wordform {verb} for {lemma}')\n",
    "                return {}, None\n",
    "            \n",
    "            if ' ' in verb: #separable verb\n",
    "                \n",
    "                verb_split = verb.split()\n",
    "                \n",
    "                if len(verb_split)!=2:\n",
    "                    #we don't treat cases with more than 1 prefix, e.g. wiederherstellen\n",
    "                    print(f'Verb morphology not identified for {wiki_record.name}')\n",
    "                    return {}, None\n",
    "\n",
    "                is_separable = True\n",
    "\n",
    "                verb, prefix = verb_split\n",
    "                verb = prefix + verb #attach the prefix to the word without any space in-between, as they are used in subordinate clauses\n",
    "\n",
    "            wordforms = generate_verbforms(verb,lemma,category) #get all possible wordforms from this word in this category\n",
    "\n",
    "            for wordform in wordforms:\n",
    "                verblemmas[wordform].add(Lemma(lemma,None,())) #assign lemma to each wordform\n",
    "\n",
    "    if 'Partizip II' in wiki_record.flexion:\n",
    "        partizip_II = get_flexion_field(wiki_record,'Partizip II')\n",
    "        hilfs_verbs = []\n",
    "        for hilfsverb_cat in ('Hilfsverb','Hilfsverb2','Hilfsverb*'):\n",
    "            hilfsverb = get_flexion_field(wiki_record, hilfsverb_cat)\n",
    "            if hilfsverb:\n",
    "                hilfs_verbs.append(hilfsverb)\n",
    "        verblemmas[partizip_II].add(Lemma(lemma,'Partizip II',tuple(hilfs_verbs)))\n",
    "        \n",
    "    if is_separable:\n",
    "        #add the zu-infinitive form used in subordinate clauses \n",
    "        verblemmas[prefix+'zu'+re.sub(f'^{prefix}','',lemma)].add(Lemma(lemma,'zu-inf',()))\n",
    "\n",
    "    return verblemmas, prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eefc988a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_verbs(dump):\n",
    "\n",
    "    verbs = defaultdict(set)\n",
    "    #partizip_II = defaultdict(set)\n",
    "    #prefixes = defaultdict(set)\n",
    "    \n",
    "    n_records = 0\n",
    "\n",
    "    parser = WiktionaryParser()\n",
    "\n",
    "    for page in dump.pages():\n",
    "        if page.redirect_to:\n",
    "            continue\n",
    "                \n",
    "        for entry in parser.entries_from_page(page):\n",
    "            wiki_record = parser.parse_entry(entry)\n",
    "            if is_german(wiki_record) and is_verb(wiki_record):\n",
    "                wordlemmas, sep_prefix = get_verb_forms(wiki_record)\n",
    "                for k,v in wordlemmas.items():\n",
    "                    verbs[k] = verbs[k].union(v)\n",
    "                #if partizip_2:\n",
    "                #    partizip_II[partizip_2].add(wiki_record.lemma.lemma)\n",
    "                #if sep_prefix:\n",
    "                #    prefixes[sep_prefix].add(wiki_record.lemma.lemma)\n",
    "                n_records += 1\n",
    "                if n_records%1000==0:\n",
    "                    print(f'{n_records} records processed')\n",
    "\n",
    "    verbs = {k:[lemma_record._asdict() for lemma_record in v] for k,v in verbs.items()} #defaultdict with sets of named tuples to dict with lists of dicts\n",
    "    #partizip_II = {k:list(v) for k,v in partizip_II.items()} #defaultdict with sets to dict with lists\n",
    "\n",
    "    return verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dfe8617-c6ac-4984-8159-f4a92f7b48f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "verbs = get_verbs(dump)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9221664f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('glemma/dataverbs.json', 'wt', encoding='UTF-8') as json_file:\n",
    "#    json.dump(verbs, json_file, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "744a08e4-4eaf-4eee-9662-0808bdda765d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_prefix(wiki_record):\n",
    "    return wiki_record.pos == {'Affix': ['Präfix']}\n",
    "\n",
    "def get_prefixes(dump):\n",
    "\n",
    "    prefixes = set()\n",
    "        \n",
    "    for page in dump.pages():\n",
    "        if page.redirect_to:\n",
    "            continue\n",
    "                \n",
    "        for entry in parser.entries_from_page(page):\n",
    "            wiki_record = parser.parse_entry(entry)\n",
    "            if is_german(wiki_record) and is_prefix(wiki_record):\n",
    "                lemma = wiki_record.lemma.lemma\n",
    "                lemma = lemma.replace('-','') #remove final - \n",
    "                if not lemma.istitle():\n",
    "                    #ignore a few prefixes starting with a capital\n",
    "                    prefixes.add(lemma)\n",
    "\n",
    "    prefixes = list(prefixes)\n",
    "            \n",
    "    return prefixes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27495b9-6eb1-44ee-93e6-7f59fe866ea6",
   "metadata": {},
   "source": [
    "# Generate adjective lookup table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "840f3ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_adjective(wiki_record):\n",
    "    return (wiki_record.pos \n",
    "            and 'Adjektiv' in wiki_record.pos \n",
    "            and wiki_record.pos['Adjektiv'] == []\n",
    "            and wiki_record.flexion is not None #we can't do much without any flexion information\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5e5edd72-87f3-4121-a24b-a3659b2cbca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_adj_declination(lemma):\n",
    "\n",
    "\n",
    "    strong_declinations = {'m':{'Nominativ Singular':'er','Genitiv Singular':'en','Dativ Singular':'em','Akkusativ Singular':'en',\n",
    "                           'Nominativ Plural':'e', 'Genitiv Plural':'er','Dativ Plural':'en', 'Akkusativ Plural':'e'},\n",
    "                           'f':{'Nominativ Singular':'e','Genitiv Singular':'er','Dativ Singular':'er','Akkusativ Singular':'e',\n",
    "                           'Nominativ Plural':'e', 'Genitiv Plural':'er','Dativ Plural':'en', 'Akkusativ Plural':'e'},\n",
    "                           'n':{'Nominativ Singular':'es','Genitiv Singular':'en','Dativ Singular':'em','Akkusativ Singular':'es',\n",
    "                           'Nominativ Plural':'e', 'Genitiv Plural':'er','Dativ Plural':'en', 'Akkusativ Plural':'e'},}\n",
    "    \n",
    "    weak_declinations = {'m':{'Nominativ Singular':'e','Genitiv Singular':'en','Dativ Singular':'en','Akkusativ Singular':'en',\n",
    "                           'Nominativ Plural':'en', 'Genitiv Plural':'en','Dativ Plural':'en', 'Akkusativ Plural':'en'},\n",
    "                           'f':{'Nominativ Singular':'e','Genitiv Singular':'en','Dativ Singular':'en','Akkusativ Singular':'e',\n",
    "                           'Nominativ Plural':'en', 'Genitiv Plural':'en','Dativ Plural':'en', 'Akkusativ Plural':'en'},\n",
    "                           'n':{'Nominativ Singular':'e','Genitiv Singular':'en','Dativ Singular':'en','Akkusativ Singular':'e',\n",
    "                           'Nominativ Plural':'en', 'Genitiv Plural':'en','Dativ Plural':'en', 'Akkusativ Plural':'en'},}\n",
    "    \n",
    "    mixed_declinations = {'m':{'Nominativ Singular':'er','Genitiv Singular':'en','Dativ Singular':'en','Akkusativ Singular':'en',\n",
    "                           'Nominativ Plural':'en', 'Genitiv Plural':'en','Dativ Plural':'en', 'Akkusativ Plural':'en'},\n",
    "                           'f':{'Nominativ Singular':'e','Genitiv Singular':'en','Dativ Singular':'en','Akkusativ Singular':'e',\n",
    "                           'Nominativ Plural':'en', 'Genitiv Plural':'en','Dativ Plural':'en', 'Akkusativ Plural':'en'},\n",
    "                           'n':{'Nominativ Singular':'es','Genitiv Singular':'en','Dativ Singular':'en','Akkusativ Singular':'es',\n",
    "                           'Nominativ Plural':'en', 'Genitiv Plural':'en','Dativ Plural':'en', 'Akkusativ Plural':'en'},}\n",
    "    \n",
    "    wordforms = []\n",
    "    \n",
    "    for declinations in (strong_declinations,weak_declinations,mixed_declinations):\n",
    "        for genus in ('m','f','n'):\n",
    "            for declination, ending in declinations[genus].items():\n",
    "                if lemma[-1]=='e':\n",
    "                    wordform = lemma[:-1] + ending\n",
    "                else:\n",
    "                    wordform = lemma + ending\n",
    "                wordforms.append((wordform, declination, genus))\n",
    "                if lemma[-2:] in ('el','en','er'):\n",
    "                    #dunkel,düster\n",
    "                    wordform = lemma[:-2] + lemma[-1] + ending \n",
    "                    wordforms.append((wordform, declination, genus))\n",
    "\n",
    "    return wordforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1439acc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_adjective_forms(wiki_record):\n",
    "\n",
    "    categories = ['Positiv', 'Komparativ', 'Superlativ', 'Positiv*', 'Komparativ*', 'Superlativ*',\n",
    "                 'Positiv**', 'Komparativ**', 'Superlativ**']\n",
    "    \n",
    "    Lemma = namedtuple('lemma', 'lemma form declination genus')\n",
    "\n",
    "    lemmas = defaultdict(set)\n",
    "\n",
    "    lemma = wiki_record.lemma.lemma.lower() #we will use lowercase lemmas in the dictionary\n",
    "\n",
    "    if ' ' in lemma:\n",
    "        #we exclude fixed expressions consisting of multiple words (often entities, e.g. 'Vereinigte Arabische Emirate')\n",
    "        return {}\n",
    "        \n",
    "    for category in categories:\n",
    "     \n",
    "        if category in wiki_record.flexion:\n",
    "                    \n",
    "            wordform = get_flexion_field(wiki_record,category).lower()\n",
    "        \n",
    "            if not re.match('^[\\w -]+$',wordform):\n",
    "                    print(f'Unrecognized characters in wordform {wordform} for {lemma}')\n",
    "                    return {}\n",
    "\n",
    "            adj_form = category.replace('*','')\n",
    "\n",
    "            if adj_form=='Superlativ':\n",
    "                wordform = re.sub('en$','',wordform)\n",
    "\n",
    "            lemmas[wordform].add(Lemma(lemma,adj_form,None,None))\n",
    "\n",
    "            for wordform, declination, genus in get_adj_declination(wordform):\n",
    "                lemmas[wordform].add(Lemma(lemma,adj_form,declination,genus))#assign lemma to the wordform\n",
    "\n",
    "\n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cb4e31c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_adjectives(dump):\n",
    "\n",
    "    adjectives = defaultdict(set)\n",
    "    \n",
    "    n_records = 0\n",
    "\n",
    "    parser = WiktionaryParser()\n",
    "\n",
    "    for page in dump.pages():\n",
    "        if page.redirect_to:\n",
    "            continue\n",
    "                \n",
    "        for entry in parser.entries_from_page(page):\n",
    "            wiki_record = parser.parse_entry(entry)\n",
    "            if is_german(wiki_record) and is_adjective(wiki_record):\n",
    "                wordlemmas = get_adjective_forms(wiki_record)\n",
    "                for k,v in wordlemmas.items():\n",
    "                    adjectives[k] = adjectives[k].union(v)\n",
    "                n_records += 1\n",
    "                if n_records%1000==0:\n",
    "                    print(f'{n_records} records processed')\n",
    "\n",
    "    adjectives = {k:[lemma_record._asdict() for lemma_record in v] for k,v in adjectives.items()} #defaultdict with sets of named tuples to dict with lists of dicts\n",
    "\n",
    "    return adjectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bc9e46a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 records processed\n",
      "Unrecognized characters in wordform d’hondtsch for d’hondtsch\n",
      "2000 records processed\n",
      "3000 records processed\n",
      "Unrecognized characters in wordform humorigg.s for humorig\n",
      "4000 records processed\n",
      "5000 records processed\n",
      "6000 records processed\n",
      "Unrecognized characters in wordform eingebildetsten<!-- for eingebildet\n",
      "7000 records processed\n",
      "Unrecognized characters in wordform ([[wenig]]) for minder\n",
      "8000 records processed\n",
      "9000 records processed\n",
      "10000 records processed\n",
      "Unrecognized characters in wordform äußer(er) for äußer-\n",
      "Unrecognized characters in wordform inner(er) for inner-\n",
      "11000 records processed\n",
      "Unrecognized characters in wordform mendel’sch for mendel’sch\n",
      "12000 records processed\n",
      "13000 records processed\n",
      "Unrecognized characters in wordform d’hondt’sch for d’hondt’sch\n",
      "14000 records processed\n",
      "Unrecognized characters in wordform 08/15 for 08/15\n"
     ]
    }
   ],
   "source": [
    "adjectives = get_adjectives(dump)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "37218427-f2e0-497d-8919-902e155ca34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lemma = namedtuple('lemma', 'lemma form declination genus')\n",
    "#\n",
    "#adj_from_verbs = defaultdict(set)\n",
    "#\n",
    "#for wordform, lemmas in verbs.items():\n",
    "#    for lemma in lemmas:\n",
    "#        if lemma['connection'] == 'Partizip II':\n",
    "#            partizip_II = wordform\n",
    "#            for declined_wordform, declination, genus in get_adj_declination(partizip_II):\n",
    "#                adj_from_verbs[declined_wordform].add(Lemma(partizip_II,'Positiv',declination,genus))\n",
    "#            partizip_I = lemma['lemma'] + 'd'\n",
    "#            for declined_wordform, declination, genus in get_adj_declination(partizip_I):\n",
    "#                adj_from_verbs[declined_wordform].add(Lemma(partizip_I,'Positiv',declination,genus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "375ef09e-154a-42e8-84c9-597a370ad234",
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_derived_forms = []\n",
    "\n",
    "for wordform, lemmas in adjectives.items():\n",
    "    for lemma in lemmas:\n",
    "        lemma_form = lemma['form']\n",
    "        if  lemma_form in ('Komparativ','Superlativ') and lemma['declination']==None:\n",
    "            lemma_from_derivative1 = re.sub('er$|est$','',wordform)\n",
    "            lemma_from_derivative2 = re.sub('r$|st$','',wordform)\n",
    "            if lemma_from_derivative1!=lemma['lemma'] and lemma_from_derivative2!=lemma['lemma']:\n",
    "                    adj_derived_forms.append(lemma_from_derivative1)   \n",
    "                    adj_derived_forms.append(lemma_from_derivative2)                    \n",
    "            adj_derived_forms.append(wordform)\n",
    "\n",
    "adj_derived_forms = set(adj_derived_forms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ad76c53a-e2bd-4317-b7b3-dc19f42721e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "adjectives = {wordform: [lemma for lemma in lemmas if not lemma['lemma'] in adj_derived_forms] for wordform, lemmas in adjectives.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "239c641d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('glemma/dataadjectives.json', 'wt', encoding='UTF-8') as json_file:\n",
    "    json.dump(adjectives, json_file, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edee3d93-5bb3-4e37-a0bf-2509418f9e1d",
   "metadata": {},
   "source": [
    "# Generate adverb lookup table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "91abd344-f920-4301-bdba-7980d2c89c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_adverb(wiki_record):\n",
    "    return (wiki_record.pos \n",
    "            and 'Adverb' in wiki_record.pos \n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "40fca8f0-6236-430b-b2df-1824801a43df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_adverbs(dump):\n",
    "\n",
    "    adverbs = set()\n",
    "    \n",
    "    n_records = 0\n",
    "\n",
    "    parser = WiktionaryParser()\n",
    "\n",
    "    for page in dump.pages():\n",
    "        if page.redirect_to:\n",
    "            continue\n",
    "                \n",
    "        for entry in parser.entries_from_page(page):\n",
    "            wiki_record = parser.parse_entry(entry)\n",
    "            if is_german(wiki_record) and is_adverb(wiki_record):\n",
    "                lemma = wiki_record.lemma.lemma.lower() #we will use lowercase lemmas in the dictionary\n",
    "                if not re.match('^[\\w -]+$',lemma):\n",
    "                    print(f'Unrecognized characters in {lemma}')\n",
    "                    continue\n",
    "                adverbs.add(lemma)\n",
    "                n_records += 1\n",
    "                if n_records%1000==0:\n",
    "                    print(f'{n_records} records processed')\n",
    "\n",
    "    return adverbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "51830383-6852-4820-8605-b67ade5185b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unrecognized characters in d’accord\n",
      "1000 records processed\n",
      "Unrecognized characters in ’naus\n"
     ]
    }
   ],
   "source": [
    "adverbs = get_adverbs(dump)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "1ae4422a-b50f-459f-b59e-78ad763ec55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "adverbs = {adv:[{'lemma':adv}] for adv in adverbs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "a847c625-db5a-4b3d-988c-fff8e0c190e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('glemma/dataadverbs.json', 'wt', encoding='UTF-8') as json_file:\n",
    "    json.dump(adverbs, json_file, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be8cd43-824d-474c-813c-03d3ee60a0b1",
   "metadata": {},
   "source": [
    "# Combine all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "22dc0cc1-59bb-48ff-977a-8c18a35bf279",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {}\n",
    "\n",
    "with open('glemma/datavocab.json', 'wt', encoding='UTF-8') as vocab_json:\n",
    "    \n",
    "    for pos, pos_dict_file in zip(('N','V','ADJ','ADV'),('nouns.json','verbs.json','adjectives.json','adverbs.json')):\n",
    "        \n",
    "        with open(f'glemma/data{pos_dict_file}', 'rt', encoding='UTF-8') as pos_json:\n",
    "            vocab[pos] = json.load(pos_json)\n",
    "            \n",
    "    json.dump(vocab, vocab_json, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744464c7-19d9-4df5-9048-30ae7b6c9cd0",
   "metadata": {},
   "source": [
    "## Test area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2a09eb98-1f75-45d5-aa98-64ef46bfa8e4",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m noun_flexion_cat \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      5\u001b[0m n_records \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m page \u001b[38;5;129;01min\u001b[39;00m dump\u001b[38;5;241m.\u001b[39mpages():\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m page\u001b[38;5;241m.\u001b[39mredirect_to:\n\u001b[1;32m      9\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m/lustre/groups/epigenereg01/workspace/projects/vale/bio/c2/resources/tools/wiktionary-de-parser/wiktionary_de_parser/dump_processor/__init__.py:134\u001b[0m, in \u001b[0;36mWiktionaryDump.pages\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m namespaces \u001b[38;5;241m=\u001b[39m {\u001b[38;5;28;01mNone\u001b[39;00m: namespace_str}\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, page_element \u001b[38;5;129;01min\u001b[39;00m etree\u001b[38;5;241m.\u001b[39miterparse(\n\u001b[1;32m    132\u001b[0m     p\u001b[38;5;241m.\u001b[39mstdout, tag\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mnamespace_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m}}\u001b[39;00m\u001b[38;5;124mpage\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    133\u001b[0m ):\n\u001b[0;32m--> 134\u001b[0m     page \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpage_element\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnamespaces\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnamespace_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m page:\n\u001b[1;32m    137\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m/lustre/groups/epigenereg01/workspace/projects/vale/bio/c2/resources/tools/wiktionary-de-parser/wiktionary_de_parser/dump_processor/__init__.py:76\u001b[0m, in \u001b[0;36mWiktionaryDump.process_data\u001b[0;34m(page_element, namespaces, namespace_ids)\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     74\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDump file extension is not .bz2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 76\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_data\u001b[39m(\n\u001b[1;32m     78\u001b[0m     page_element, namespaces: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28mstr\u001b[39m], namespace_ids: \u001b[38;5;28mset\u001b[39m[\u001b[38;5;28mint\u001b[39m]\n\u001b[1;32m     79\u001b[0m ):\n\u001b[1;32m     80\u001b[0m     page_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(page_element\u001b[38;5;241m.\u001b[39mfindtext(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, namespaces))\n\u001b[1;32m     81\u001b[0m     title \u001b[38;5;241m=\u001b[39m page_element\u001b[38;5;241m.\u001b[39mfindtext(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, namespaces)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "parser = WiktionaryParser()\n",
    "    \n",
    "noun_flexion_cat = []\n",
    "\n",
    "n_records = 0\n",
    "\n",
    "for page in dump.pages():\n",
    "    if page.redirect_to:\n",
    "        continue\n",
    "        \n",
    "    #page_names.append(page.name)\n",
    "                \n",
    "    if page.name in (\"spätest\",):\n",
    "        for entry in parser.entries_from_page(page):\n",
    "            wiki_record = parser.parse_entry(entry)\n",
    "            if is_german(wiki_record):# and is_adjective(wiki_record):\n",
    "                print(wiki_record)\n",
    "                #adj_forms = get_adjective_forms(wiki_record)\n",
    "                #keys = tuple([x for x in wiki_record.flexion.keys() if x[0].isupper()])\n",
    "                #noun_flexion_cat.append((page.name,keys))\n",
    "                n_records += 1\n",
    "                \n",
    "            #pprint(wiki_record)\n",
    "            #break\n",
    "        #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "782128c3-8390-442e-abed-95deb4a81dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "words, cats = zip(*noun_flexion_cat)\n",
    "\n",
    "words = np.array(words)\n",
    "cats_flatten = pd.Series([y for x in cats for y in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5764e486-0da1-4f01-ab3f-ceac4299f214",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Positiv', 'Komparativ', 'Superlativ', 'Komparativ*',\n",
       "       'Superlativ*', 'Komparativ**', 'Superlativ**'], dtype=object)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cats_flatten.unique()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
