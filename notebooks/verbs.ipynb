{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da6f45f6-3a1d-4659-9ae1-9c08295a7754",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import re\n",
    "from collections import defaultdict,namedtuple\n",
    "import json\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "\n",
    "parser_dir = '/lustre/groups/epigenereg01/workspace/projects/vale/bio/c2/resources/tools/wiktionary-de-parser'\n",
    "sys.path.append(parser_dir)\n",
    "\n",
    "from wiktionary_de_parser.dump_processor import WiktionaryDump\n",
    "from wiktionary_de_parser import WiktionaryParser\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27495b9-6eb1-44ee-93e6-7f59fe866ea6",
   "metadata": {},
   "source": [
    "# Generate verb lookup table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a8eb563-6585-425f-b26c-9368d0332930",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_german(wiki_record):\n",
    "    return wiki_record.language.lang == 'Deutsch'\n",
    "\n",
    "def get_flexion_field(wiki_record, field_name):\n",
    "    if field_name in wiki_record.flexion:\n",
    "        wordform = wiki_record.flexion[field_name].strip()\n",
    "        wordform = re.sub(r\".*:'' \",r\"\",wordform) #remarks like 'selten:'/'militarisch:'\n",
    "        return wordform\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7c90bf1-b74b-4e0c-89d9-72f189134e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_categories = ['Präsens_ich', 'Präsens_du', 'Präsens_er, sie, es', 'Präteritum_ich', 'Konjunktiv II_ich', \n",
    "                     'Imperativ Singular', 'Imperativ Plural']\n",
    "\n",
    "categories = [x+'*'*n for x in base_categories for n in range(4)] + ['Imperativ Singular 2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0de3be6-1705-4ad1-b0d5-3aca06bd19ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_verb(wiki_record):\n",
    "    return (wiki_record.pos \n",
    "            and 'Verb' in wiki_record.pos \n",
    "            and wiki_record.pos['Verb'] in ([],['Hilfsverb'])\n",
    "            and wiki_record.flexion is not None #we can't do much without any flexion information\n",
    "           )\n",
    "\n",
    "def stem_verb(verb):\n",
    "    root = re.sub(r'e?n$|e$|e?t$|est$|([^s])st$',r'\\1', verb) #remove ending, leave s when it's part of the root, e.g. lassen->lass \n",
    "    return root\n",
    "\n",
    "def generate_verbforms(verb, lemma, category):\n",
    "    '''\n",
    "    Add verb forms that are usually absent in the Wiktionary flexion entry\n",
    "    '''\n",
    "    wordforms = [verb,]\n",
    "    \n",
    "    if category.startswith('Präsens_ich'):\n",
    "\n",
    "        if lemma == 'sein':\n",
    "\n",
    "            wordforms += ['sei','seiest','seist','seiet', 'seien', 'sind'] #Konjunktiv I, Indikativ 3. Person Plural\n",
    "\n",
    "        elif lemma in ('können', 'sollen', 'müssen', 'dürfen', 'wollen', 'mögen'):\n",
    "\n",
    "            wordforms += [lemma[:-1],lemma[:-1]+'st',lemma[:-1]+'t', lemma[:-2]+'t']\n",
    "            \n",
    "        elif lemma.endswith('ern') or lemma.endswith('eln'): \n",
    "            #wandern, sammeln\n",
    "            if verb.endswith('ere') or verb.endswith('ele'):\n",
    "                wordforms += [verb[:-1]+'st', verb[:-1]+'t']  #Indikativ 2. Person Plural, Konjunktiv I 2. Person Singular, Konjunktiv I 2. Person Plural\n",
    "            elif verb.endswith('le'):\n",
    "                wordforms += [verb+'st', verb+'t']  #Konjunktiv I 2. Person Singular, Konjunktiv I 2. Person Plural, alternative forms\n",
    "        else:\n",
    "\n",
    "            wordforms += [verb+'st', verb+'t']  #Konjunktiv I 2. Person Singular, Konjunktiv I 2. Person Plural\n",
    "                \n",
    "            verb = re.sub(r'([^td])e$|([wtzpsdfghjkxvb][mn]e)$',r'\\1\\2',verb) #don't remove e if preceeded by t or d or m,n after a consonant (except l,r,l,m)\n",
    "                \n",
    "            wordforms += [verb+'st', verb+'t']  #Indikativ 2. Person Plural\n",
    "\n",
    "        if lemma.endswith('auern'):\n",
    "            #bedauern, kauern\n",
    "            wordforms += [re.sub(r'auere$',r'aure',verb)] #bedauere-->bedaure\n",
    "\n",
    "        if lemma == 'werden':\n",
    "            wordforms += ['worden']\n",
    "           \n",
    "    elif category.startswith('Präteritum_ich'):\n",
    "        \n",
    "        if verb[-1]=='e':\n",
    "            #regular + mixed verbs\n",
    "            #machen, denken\n",
    "            wordforms += [verb+'st', verb+'t', verb+'n']\n",
    "        elif verb[-1] in ('s','ß','z'):\n",
    "            #lassen,schmelzen,blasen\n",
    "            wordforms += [verb+'est', verb+'t', verb+'en']  \n",
    "        elif re.search(r'([td]$)|([wtzpsdfghjkxvb][mn]$)', verb):\n",
    "            #halten, finden\n",
    "            wordforms += [verb+'est', verb+'st', verb+'et', verb+'en']  \n",
    "        else:\n",
    "            #irregular verbs\n",
    "            #sprechen\n",
    "            wordforms += [verb+'st', verb+'t', verb+'en']  \n",
    "\n",
    "    elif category.startswith('Konjunktiv II_ich'):\n",
    "\n",
    "        if lemma=='fahren':\n",
    "            #don't add Konjunktiv II for 'fahren' to avoid confusuion with Indikativ for 'führen'\n",
    "            return []\n",
    "            \n",
    "        wordforms += [verb+'st', verb+'t', verb+'n'] #Konjunktiv II 2. Person Singular, Konjunktiv II 2.,3. Person Plural\n",
    "            \n",
    "    return wordforms\n",
    "    \n",
    "def get_verb_forms(wiki_record):\n",
    "\n",
    "    Lemma = namedtuple('lemma', 'lemma connection via')\n",
    "\n",
    "    verblemmas = defaultdict(set)\n",
    "\n",
    "    lemma = wiki_record.lemma.lemma #word lemma\n",
    "    \n",
    "    if ' ' in lemma:\n",
    "        #we ignore the cases where the prefix is not attached to the verb in subordinate clauses\n",
    "        #e.g. frei geben, bekannt machen\n",
    "        #dependency parses like SpaCy can't recognize that these are both parts of the same verb anyway\n",
    "        return {}, None\n",
    "\n",
    "    verblemmas[lemma].add(Lemma(lemma,None,()))\n",
    "\n",
    "    is_separable, prefix = False, None\n",
    "\n",
    "    for category in categories:\n",
    "        \n",
    "        #category with asterisk for alternative forms, e.g. ich anerkenne, ich erkenne an\n",
    "        \n",
    "        if category in wiki_record.flexion:\n",
    "            \n",
    "            verb = get_flexion_field(wiki_record,category)\n",
    "\n",
    "            if not re.match('^[\\w ]+$',verb):\n",
    "                print(f'Unrecognized characters in wordform {verb} for {lemma}')\n",
    "                return {}, None\n",
    "            \n",
    "            if ' ' in verb: #separable verb\n",
    "                \n",
    "                verb_split = verb.split()\n",
    "                \n",
    "                if len(verb_split)!=2:\n",
    "                    #we don't treat cases with more than 1 prefix, e.g. wiederherstellen\n",
    "                    print(f'Verb morphology not identified for {wiki_record.name}')\n",
    "                    return {}, None\n",
    "\n",
    "                is_separable = True\n",
    "\n",
    "                verb, prefix = verb_split\n",
    "                verb = prefix + verb #attach the prefix to the word without any space in-between, as they are used in subordinate clauses\n",
    "\n",
    "            wordforms = generate_verbforms(verb,lemma,category) #get all possible wordforms from this word in this category\n",
    "\n",
    "            for wordform in wordforms:\n",
    "                verblemmas[wordform].add(Lemma(lemma,None,())) #assign lemma to each wordform\n",
    "\n",
    "    if 'Partizip II' in wiki_record.flexion:\n",
    "        partizip_II = get_flexion_field(wiki_record,'Partizip II')\n",
    "        hilfs_verbs = []\n",
    "        for hilfsverb_cat in ('Hilfsverb','Hilfsverb2','Hilfsverb*'):\n",
    "            hilfsverb = get_flexion_field(wiki_record, hilfsverb_cat)\n",
    "            if hilfsverb:\n",
    "                hilfs_verbs.append(hilfsverb)\n",
    "        verblemmas[partizip_II].add(Lemma(lemma,'Partizip II',tuple(hilfs_verbs)))\n",
    "        \n",
    "    if is_separable:\n",
    "        #add the zu-infinitive form used in subordinate clauses \n",
    "        verblemmas[prefix+'zu'+re.sub(f'^{prefix}','',lemma)].add(Lemma(lemma,'zu-inf',()))\n",
    "\n",
    "    return verblemmas, prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc4889ed-20e4-4b17-99e9-f769d4bdcb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_prefix(wiki_record):\n",
    "    return wiki_record.pos == {'Affix': ['Präfix']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "663efa4a-2e31-46b5-aa4c-9db4e825c446",
   "metadata": {},
   "outputs": [],
   "source": [
    "dump = WiktionaryDump(\n",
    "    dump_file_path=parser_dir + \"/wiktionary_german/dewiktionary-latest-pages-articles-multistream.xml.bz2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "beb5a53b-d781-4171-8ddc-60276399bab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_verbs(dump):\n",
    "\n",
    "    verbs = defaultdict(set)\n",
    "    #partizip_II = defaultdict(set)\n",
    "    #prefixes = defaultdict(set)\n",
    "    \n",
    "    n_records = 0\n",
    "\n",
    "    parser = WiktionaryParser()\n",
    "\n",
    "    for page in dump.pages():\n",
    "        if page.redirect_to:\n",
    "            continue\n",
    "                \n",
    "        for entry in parser.entries_from_page(page):\n",
    "            wiki_record = parser.parse_entry(entry)\n",
    "            if is_german(wiki_record) and is_verb(wiki_record):\n",
    "                wordlemmas, sep_prefix = get_verb_forms(wiki_record)\n",
    "                for k,v in wordlemmas.items():\n",
    "                    verbs[k] = verbs[k].union(v)\n",
    "                #if partizip_2:\n",
    "                #    partizip_II[partizip_2].add(wiki_record.lemma.lemma)\n",
    "                #if sep_prefix:\n",
    "                #    prefixes[sep_prefix].add(wiki_record.lemma.lemma)\n",
    "                n_records += 1\n",
    "                if n_records%1000==0:\n",
    "                    print(f'{n_records} records processed')\n",
    "\n",
    "    verbs = {k:[lemma_record._asdict() for lemma_record in v] for k,v in verbs.items()} #defaultdict with sets of named tuples to dict with lists of dicts\n",
    "    #partizip_II = {k:list(v) for k,v in partizip_II.items()} #defaultdict with sets to dict with lists\n",
    "\n",
    "    return verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "744a08e4-4eaf-4eee-9662-0808bdda765d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prefixes(dump):\n",
    "\n",
    "    prefixes = set()\n",
    "        \n",
    "    for page in dump.pages():\n",
    "        if page.redirect_to:\n",
    "            continue\n",
    "                \n",
    "        for entry in parser.entries_from_page(page):\n",
    "            wiki_record = parser.parse_entry(entry)\n",
    "            if is_german(wiki_record) and is_prefix(wiki_record):\n",
    "                lemma = wiki_record.lemma.lemma\n",
    "                lemma = lemma.replace('-','') #remove final - \n",
    "                if not lemma.istitle():\n",
    "                    #ignore a few prefixes starting with a capital\n",
    "                    prefixes.add(lemma)\n",
    "\n",
    "    prefixes = list(prefixes)\n",
    "            \n",
    "    return prefixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5dfe8617-c6ac-4984-8159-f4a92f7b48f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verb morphology not identified for spulgen\n",
      "Unrecognized characters in wordform -fiziere for -fizier\n",
      "Unrecognized characters in wordform -ele for -el\n",
      "1000 records processed\n",
      "Unrecognized characters in wordform -ige for -ig\n",
      "2000 records processed\n",
      "Verb morphology not identified for wiederherstellen\n",
      "3000 records processed\n",
      "4000 records processed\n",
      "5000 records processed\n",
      "Unrecognized characters in wordform -iere for -ier\n",
      "Unrecognized characters in wordform e-maile for e-mailen\n",
      "Unrecognized characters in wordform (schneibte) for schneiben\n",
      "Unrecognized characters in wordform -ze for -zen\n",
      "Unrecognized characters in wordform (dergab) for dergeben\n",
      "Unrecognized characters in wordform (sprang umhin) for umhinspringen\n",
      "Unrecognized characters in wordform (derfror) for derfrieren\n",
      "Unrecognized characters in wordform (bätzte) for bätzen\n",
      "Unrecognized characters in wordform (bätzte aus) for ausbätzen\n",
      "Unrecognized characters in wordform (bätzte anhin) for anhinbätzen\n",
      "Unrecognized characters in wordform (zerbätzte) for zerbätzen\n",
      "Unrecognized characters in wordform dersauf! for dersaufen\n",
      "Unrecognized characters in wordform (derkrallte) for derkrallen\n",
      "Unrecognized characters in wordform (derkam) for derkemmen\n",
      "Unrecognized characters in wordform (derfing) for derfangen\n",
      "Unrecognized characters in wordform (derwischte) for derwischen\n",
      "Unrecognized characters in wordform (derwuzelte) for derwuzeln\n",
      "Unrecognized characters in wordform (schweibte abhin) for abhinschweiben\n",
      "Unrecognized characters in wordform (beutelte abher) for abherbeuteln\n",
      "Unrecognized characters in wordform (pappte aufhin) for aufhinpappen\n",
      "Unrecognized characters in wordform (schloff ausher) for ausherschliefen\n",
      "6000 records processed\n",
      "Unrecognized characters in wordform dersäuf! for dersäufen\n",
      "Unrecognized characters in wordform (schweibte) for schweiben\n",
      "7000 records processed\n",
      "Verb morphology not identified for wiederaufbereiten\n",
      "8000 records processed\n",
      "Verb morphology not identified for wiedergutmachen\n",
      "Unrecognized characters in wordform (derbarmte) for derbarmen\n",
      "Unrecognized characters in wordform (blattelte ab) for abblatteln\n",
      "9000 records processed\n",
      "Verb morphology not identified for wiedereingliedern\n",
      "10000 records processed\n",
      "Verb morphology not identified for wiederauferstehen\n",
      "11000 records processed\n",
      "12000 records processed\n",
      "Verb morphology not identified for wiederaufbauen\n",
      "Verb morphology not identified for mitansehen\n",
      "Verb morphology not identified for mitangeben\n",
      "Verb morphology not identified for fehlauffassen\n",
      "13000 records processed\n",
      "Verb morphology not identified for wiederaufnehmen\n",
      "Unrecognized characters in wordform (derstieß) for derstessen\n",
      "14000 records processed\n"
     ]
    }
   ],
   "source": [
    "verbs = get_verbs(dump)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "95e29f60-4dea-4bac-98cb-7482f1d3e97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/verbs.json', 'wt', encoding='UTF-8') as json_file:\n",
    "    json.dump(verbs, json_file, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce1efe0-fc98-40e9-b237-9d738e9f5444",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prefixes = get_prefixes(dump)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb1fdeb-29c4-466e-848c-cffbe4594c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('data/prefixes.json', 'wt', encoding='UTF-8') as json_file:\n",
    "#    json.dump(prefixes, json_file, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744464c7-19d9-4df5-9048-30ae7b6c9cd0",
   "metadata": {},
   "source": [
    "# Unknown verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "54dae89e-9fdd-45b5-a77b-94d3c1843ff4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "2a09eb98-1f75-45d5-aa98-64ef46bfa8e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParsedWiktionaryPageEntry(name='entwickeln', hyphenation=['ent', 'wi', 'ckeln'], flexion={'Präsens_ich': 'entwickle', 'Präsens_du': 'entwickelst', 'Präsens_er, sie, es': 'entwickelt', 'Präteritum_ich': 'entwickelte', 'Partizip II': 'entwickelt', 'Konjunktiv II_ich': 'entwickelte', 'Imperativ Singular': 'entwickle', 'Imperativ Plural': 'entwickelt', 'Hilfsverb': 'haben'}, ipa=['ɛntˈvɪkl̩n'], language=Language(lang='Deutsch', lang_code='de'), lemma=Lemma(lemma='entwickeln', inflected=False), pos={'Verb': []}, rhymes=['ɪkl̩n'])\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "parser = WiktionaryParser()\n",
    "    \n",
    "page_names = []\n",
    "\n",
    "verbs = defaultdict(set)\n",
    "prefixes = defaultdict(set)\n",
    "\n",
    "n_records = 0\n",
    "\n",
    "for page in dump.pages():\n",
    "    if page.redirect_to:\n",
    "        continue\n",
    "        \n",
    "    page_names.append(page.name)\n",
    "                \n",
    "    if page.name in (\"entwickeln\",):\n",
    "        for entry in parser.entries_from_page(page):\n",
    "            wiki_record = parser.parse_entry(entry)\n",
    "            pprint(wiki_record)\n",
    "            print('Hilfsverb' in wiki_record.flexion)\n",
    "            break\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c677558b-15ad-44bf-93bd-7e1625fcb595",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/verbs.json', 'rt', encoding='UTF-8') as json_file:\n",
    "    verbs = json.load(json_file)\n",
    "\n",
    "with open('data/prefixes.json', 'rt', encoding='UTF-8') as json_file:\n",
    "    prefixes = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "14600d4e-ab4b-483a-85cf-46e8282cd051",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in verbs.items():\n",
    "    if len(v)>1:\n",
    "        print(k,v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3006f7-e15a-4ebb-8c96-d8d867c84cb3",
   "metadata": {},
   "source": [
    "# Lemmatizer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "001eacf1-26d4-4318-90cb-9d6c8fc1e5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_verb_lemma_morpho(word, query_verb_dict_fnc, prefixes_list, use_longest_subword=True):\n",
    "\n",
    "    def get_verb_lemma_recursive(word, morphemes=[]):\n",
    "        '''\n",
    "        Recursively detect prefixes and yield all possible lemmas\n",
    "        '''\n",
    "        res = []\n",
    "        \n",
    "        base_lemma = query_verb_dict_fnc(word)\n",
    "        \n",
    "        if base_lemma:\n",
    "            #print(word,morphemes,base_lemma)\n",
    "            if len(morphemes)>1 and morphemes[-1] == 'zu' and base_lemma == word:\n",
    "                #suspect a zu-infinitive: some prefix on the right + zu + infinitive\n",
    "                #e.g. abzuheben, aufzuatmen\n",
    "                res.append((''.join(morphemes[:-1]+[base_lemma]),word)) #add without \"zu\"\n",
    "            else:\n",
    "                res.append((''.join(morphemes+[base_lemma]),word)) #compose the infinitive out of the collected prefixes and the base lemma\n",
    "                \n",
    "        for prefix_end_idx in range(1,len(word)-2):\n",
    "            #look for the next prefix which ends at prefix_end_idx\n",
    "            if word[:prefix_end_idx] in prefixes_list:\n",
    "                #prefix in the list of known prefixes\n",
    "                prefix = word[:prefix_end_idx] \n",
    "                res.extend(get_verb_lemma_recursive(word[prefix_end_idx:], morphemes+[prefix])) #detach the prefix, call the function again\n",
    "    \n",
    "        return res\n",
    "        \n",
    "    lemmas = get_verb_lemma_recursive(word)\n",
    "\n",
    "    if lemmas:\n",
    "        lemmas.sort(key=lambda x:-len(x[1])) #sort according to subword length, ascending=False\n",
    "        lemmas, subwords = zip(*lemmas)     \n",
    "        if (use_longest_subword or len(set(lemmas))==1):\n",
    "        #all possible splits lead to the same lemma or taking the longest subword allowed\n",
    "            return lemmas[0]\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "23351203-fff8-4e1f-9257-73426b6719e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_verb_lemma_fwdsearch(word, query_verb_dict_fnc, use_longest_subword=True):\n",
    "\n",
    "    '''\n",
    "    Get all possible verb lemmas using forward search, prefix-agnostic\n",
    "    '''\n",
    "\n",
    "    lemmas = []\n",
    "    \n",
    "    for start_idx in range(0,len(word)-2):\n",
    "        #remove letters one by one, until the rest of the word matches one in the vocabulary\n",
    "        trial_word = word[start_idx:]\n",
    "        base_lemma = query_verb_dict_fnc(trial_word)\n",
    "        if start_idx>2 and word[start_idx-2:start_idx] == 'zu' and base_lemma == trial_word:\n",
    "            #suspect a zu-infinitive: some prefix on the right + zu + infinitive\n",
    "            #e.g. abzuheben, aufzuatmen\n",
    "            lemmas.append(word[0:start_idx-2] + base_lemma) #add without \"zu\"\n",
    "        elif base_lemma and not base_lemma.startswith('zu'):\n",
    "            lemmas.append(word[0:start_idx] + base_lemma)\n",
    "\n",
    "    if lemmas and (use_longest_subword or len(set(lemmas))==1):\n",
    "        #all possible splits lead to the same lemma or taking the longest subword allowed\n",
    "        return lemmas[0]\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0b304527-c0d6-424c-948b-56a3371c5b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GLemma():\n",
    "\n",
    "    \"\"\"Wiktionary-based German lemmatizer.\n",
    "\n",
    "    Provides a lemma for a given word given the POS tag:\n",
    "    NOUN, VERB, ADJ, ADV.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    lemmatizer_data_path : str\n",
    "        Data path to the lemmatizer dictionaries.\n",
    "        \n",
    "    unknown_strategy : str\n",
    "        How to treat words not represented in the dictionary.\n",
    "        When contains {POS}_morpho string, performs morphological analysis of the given word\n",
    "        by decomposing it into known parts: a verb is split into its prefixes and the root and\n",
    "        the compound noun is split into more simple nouns. When contains {POS}_fwds string,\n",
    "        looks for all the words that are contained in the given word and uses their lemmas.\n",
    "        When multiple lemmas for the given word are possible, returns None unless the longest_subword_ukn flag is True.\n",
    "\n",
    "    longest_subword_ukn : bool, default=True\n",
    "        Use the lemma of the longest subword for unknown words.\n",
    "        \n",
    "    wordfreq_csv : str, default=None\n",
    "        A file with approximate word frequencies. When multiple lemmas\n",
    "        for a given word form are possible, the most frequent lemma is taken.\n",
    "        Does not have to be a lemma list.\n",
    "    \n",
    "    Examples\n",
    "    --------\n",
    "    >>> lemmatizer = GLemma('./data', unknown='VERB_morpho;VERB_fwds',\n",
    "                    wordfreq_csv='data/third-party/FrequencyWords/content/2018/de/de_full.txt')\n",
    "    >>> lemmatizer('vermalt','VERB')\n",
    "    'vermalen'\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    If a verb prefix is separated in the sentence, it should be attached to the root before the lemmatization:\n",
    "    Ich hole dich ab --> lemmatizer('abhole','VERB')\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, lemmatizer_data_path, unknown_strategy='skip', longest_subword_ukn=True, wordfreq_csv=None):\n",
    "\n",
    "        with open(os.path.join(lemmatizer_data_path,'verbs.json'), 'rt', encoding='UTF-8') as json_file:\n",
    "            self.verbs = json.load(json_file)\n",
    "        \n",
    "        with open(os.path.join(lemmatizer_data_path,'prefixes.json'), 'rt', encoding='UTF-8') as json_file:\n",
    "            self.prefixes = json.load(json_file)\n",
    "    \n",
    "        if wordfreq_csv:\n",
    "            self.wordfreq = pd.read_csv(wordfreq_csv, sep=' ', names=['word','freq']).set_index('word').freq.sort_values(ascending=False) #sort by frequency\n",
    "            self.wordfreq = self.wordfreq.to_dict()\n",
    "        else:\n",
    "            self.wordfreq = None\n",
    "\n",
    "        self.unknown_strategy = unknown_strategy\n",
    "        self.longest_subword_ukn = longest_subword_ukn\n",
    "\n",
    "    def get_most_frequent_word(self, lemmalist):\n",
    "        \n",
    "        if self.wordfreq:\n",
    "            freqs = [self.wordfreq.get(word, np.nan) for word in lemmalist]\n",
    "            if all(np.isnan(freqs)):\n",
    "                return None\n",
    "            else:\n",
    "                #if at least one word in the wordrank dictionary\n",
    "                return lemmalist[np.nanargmax(freqs)]\n",
    "        else:\n",
    "            return None\n",
    "            \n",
    "    def get_verb_lemma(self, verb, spacy_token=None):\n",
    "        \n",
    "        lemmas = self.verbs.get(verb, None)  \n",
    "        \n",
    "        if not lemmas:\n",
    "            #maybe old orthography? try to replace ß with ss at the end of the stem \n",
    "            newform=re.sub(r'ß($|t?en$|t?e$|t?e?t$|t?est$)',r'ss\\1', verb) \n",
    "            lemmas = self.verbs.get(newform, None)\n",
    "            \n",
    "        if not lemmas:\n",
    "            return None\n",
    "            \n",
    "        n_unique_lemmas = len(set([x['lemma'] for x in lemmas])) #count unique lemmas, e.g. 'konzentriert' will have to records: one for the infinitive and one for the Partizip II\n",
    "        \n",
    "        if n_unique_lemmas>1 and spacy_token is not None:\n",
    "            \n",
    "            if spacy_token.head.lemma_ in ('haben','sein'):\n",
    "                #Perfekt suspected\n",
    "                \n",
    "                n_hilfsverb = len(set([y for x in lemmas for y in x['via']]))\n",
    "                \n",
    "                if n_hilfsverb>1: #do we really have to choose between sein and haben?\n",
    "                    lemmas = [x for x in lemmas if x['connection']=='Partizip II' \n",
    "                                    and spacy_token.head.lemma_ in x['via']]\n",
    "                    \n",
    "            elif spacy_token.head.lemma_=='werden':\n",
    "                #Passiv or Futur suspected\n",
    "                \n",
    "                lemmas = [x for x in lemmas if x['connection']=='Partizip II' \n",
    "                                or x['lemma']==verb]\n",
    "                \n",
    "            else:\n",
    "                #no evidence for Perfekt or Passiv\n",
    "                \n",
    "                lemmas = [x for x in lemmas if not x['connection']=='Partizip II']\n",
    "                \n",
    "        lemmas = list(set([x['lemma'] for x in lemmas])) #remove all meta info, take unique lemmas\n",
    "        \n",
    "        if not lemmas:\n",
    "            \n",
    "            return None\n",
    "            \n",
    "        elif len(lemmas)>1:\n",
    "\n",
    "            return self.get_most_frequent_word(lemmas)\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            return lemmas[0]\n",
    "\n",
    "    def __call__(self, word=None, pos=None, spacy_token=None):\n",
    "\n",
    "        lemma = None\n",
    "\n",
    "        if not word:\n",
    "            word, pos = spacy_token.text, spacy_token.pos_\n",
    "        \n",
    "        if pos.startswith('V'):\n",
    "            word = word.lower()\n",
    "            lemma = self.get_verb_lemma(word, spacy_token=spacy_token)\n",
    "            if not lemma:\n",
    "                #if the given wordform not found in the dictionary\n",
    "                if 'VERB_morpho' in self.unknown_strategy:\n",
    "                    #first try to get the lemma by decomposing the word into the prefixes and the root\n",
    "                    lemma = get_verb_lemma_morpho(word, lambda x:self.get_verb_lemma(x, spacy_token=spacy_token), self.prefixes, use_longest_subword=self.longest_subword_ukn) \n",
    "                if not lemma and 'VERB_fwds' in self.unknown_strategy:\n",
    "                    #then try to get the lemma by finding a subword that is in the dictionary\n",
    "                    lemma = get_verb_lemma_fwdsearch(word, lambda x:self.get_verb_lemma(x, spacy_token=spacy_token), use_longest_subword=self.longest_subword_ukn)\n",
    "\n",
    "        return lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1569d82a-c9cb-42a4-995b-aeb96b7a2c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = GLemma('./data', unknown_strategy='VERB_fwds',\n",
    "                    wordfreq_csv='data/third-party/FrequencyWords/content/2018/de/de_full.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "992e2884-8e9e-49f3-bdf3-eea5e8fec11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spacy_model(language='en'):\n",
    "\n",
    "    import spacy\n",
    "\n",
    "    language = {\n",
    "        'en': 'en_core_web_lg',\n",
    "        'fr': 'french',\n",
    "        'de': 'de_core_news_lg',\n",
    "    }[language]\n",
    "    return spacy.load(language) \n",
    "\n",
    "spacy_model = get_spacy_model('de')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4a6640-3dce-4710-91e5-f9fe0e7a2556",
   "metadata": {},
   "source": [
    "## Test TIGER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1fed9cd5-e63e-4c03-9263-9a0fc0ca5731",
   "metadata": {},
   "outputs": [],
   "source": [
    "tiger_dataset = 'data/third-party/tiger_release_aug07.corrected.16012013.xml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "537b4358-a37c-41ea-a064-684bb00c2968",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tiger():\n",
    "    \n",
    "    sentences = []\n",
    "\n",
    "    with open(tiger_dataset,'r', encoding='iso-8859-15') as f:\n",
    "        while True:\n",
    "            line = f.readline()\n",
    "            while not '<terminals>' in line:\n",
    "                line = f.readline()\n",
    "                if '</corpus>' in line:\n",
    "                    return sentences\n",
    "            words = []\n",
    "            while not '</terminals>' in line:\n",
    "                line = f.readline()\n",
    "                s = re.search(r'word=\"(\\w+)\" lemma=\"(\\w+)\" pos=\"(\\w+)\"',line)\n",
    "                if s:\n",
    "                    words.append(s.groups(0))\n",
    "            if len(words)>0:\n",
    "                sentences.append(words)\n",
    "\n",
    "tiger_sentences = read_tiger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9e278e32-72ba-476a-a0f4-76f48afb4ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1999\n"
     ]
    }
   ],
   "source": [
    "tiger_res = []\n",
    "\n",
    "for idx,sentence in enumerate(tiger_sentences):\n",
    "    words, lemmas, pos = zip(*sentence)\n",
    "    text = ' '.join(words)\n",
    "    doc = spacy_model(text)\n",
    "    if len(doc)==len(lemmas):\n",
    "        for token, tiger_word, tiger_lemma, tiger_pos in zip(doc,words,lemmas,pos):\n",
    "             if tiger_word == token.text:\n",
    "                if token.pos_=='VERB':\n",
    "                   lemma = lemmatizer(spacy_token=token)\n",
    "                   tiger_res.append((text,tiger_word, tiger_pos, tiger_lemma, token.pos_, token.lemma_, lemma))\n",
    "    if (idx+1)%2000==0:\n",
    "        print(idx)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f59f86e5-a844-4555-b3a3-91886d3c80b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tiger_res = pd.DataFrame(tiger_res, columns = ['sentence','word','tiger_pos','tiger_lemma','spacy_pos','spacy_lemma','pred_lemma'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8aa77a6b-0dd4-4a29-9ec0-2107a13b3178",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9979570990806946"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((tiger_res.tiger_pos.str.startswith('V')) & (tiger_res.spacy_pos=='VERB')).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "911ef0aa-5371-4d7c-8b78-58d7f5f563d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.993190330268982"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(tiger_res.tiger_lemma==tiger_res.pred_lemma).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "99696c67-a449-4417-8da2-43202ed4ef50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.989445011916922"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(tiger_res.tiger_lemma==tiger_res.spacy_lemma).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "a187e251-5f57-4d70-9fd1-23465500e89b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>word</th>\n",
       "      <th>tiger_pos</th>\n",
       "      <th>tiger_lemma</th>\n",
       "      <th>spacy_pos</th>\n",
       "      <th>spacy_lemma</th>\n",
       "      <th>pred_lemma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Den Herren Rao und Singh gebührt ein Platz in ...</td>\n",
       "      <td>resumiert</td>\n",
       "      <td>VVFIN</td>\n",
       "      <td>resumieren</td>\n",
       "      <td>VERB</td>\n",
       "      <td>resumieren</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>526</th>\n",
       "      <td>Zur Bedarfsdeckung müßten bis 2000 rund 90 000...</td>\n",
       "      <td>hinzukommen</td>\n",
       "      <td>VVINF</td>\n",
       "      <td>hinzukommen</td>\n",
       "      <td>VERB</td>\n",
       "      <td>hinzukommen</td>\n",
       "      <td>hinkommen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>Erst mit Hilfe der aus anderen Orten herbeigee...</td>\n",
       "      <td>gelang</td>\n",
       "      <td>VVFIN</td>\n",
       "      <td>gelingen</td>\n",
       "      <td>VERB</td>\n",
       "      <td>gelingen</td>\n",
       "      <td>gelangen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1008</th>\n",
       "      <td>Doch die meisten der nach der Wende rund 4500 ...</td>\n",
       "      <td>verschuldet</td>\n",
       "      <td>ADJD</td>\n",
       "      <td>verschuldet</td>\n",
       "      <td>VERB</td>\n",
       "      <td>verschulden</td>\n",
       "      <td>verschulden</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1147</th>\n",
       "      <td>Als positiv führen die Experten an daß Versorg...</td>\n",
       "      <td>führen</td>\n",
       "      <td>VVFIN</td>\n",
       "      <td>fahren</td>\n",
       "      <td>VERB</td>\n",
       "      <td>fahren</td>\n",
       "      <td>führen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1202</th>\n",
       "      <td>Aber dann spannt der Gigant die Muskeln die Fe...</td>\n",
       "      <td>spannt</td>\n",
       "      <td>VVFIN</td>\n",
       "      <td>spannen</td>\n",
       "      <td>VERB</td>\n",
       "      <td>spannen</td>\n",
       "      <td>spinnen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1228</th>\n",
       "      <td>Wie nicht anders zu erwarten rutschten die mei...</td>\n",
       "      <td>dotierte</td>\n",
       "      <td>ADJA</td>\n",
       "      <td>dotiert</td>\n",
       "      <td>VERB</td>\n",
       "      <td>dotiert</td>\n",
       "      <td>dotieren</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1278</th>\n",
       "      <td>Von den Banken gelang es während der vergangen...</td>\n",
       "      <td>gelang</td>\n",
       "      <td>VVFIN</td>\n",
       "      <td>gelingen</td>\n",
       "      <td>VERB</td>\n",
       "      <td>gelingen</td>\n",
       "      <td>gelangen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1436</th>\n",
       "      <td>Der Münsteraner Theologe Tiemo Rainer Peters p...</td>\n",
       "      <td>prangert</td>\n",
       "      <td>VVFIN</td>\n",
       "      <td>prangern</td>\n",
       "      <td>VERB</td>\n",
       "      <td>prangern</td>\n",
       "      <td>prangeren</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1437</th>\n",
       "      <td>Einen Gleichklang zwischen verstärkten Tendenz...</td>\n",
       "      <td>prangert</td>\n",
       "      <td>VVFIN</td>\n",
       "      <td>prangern</td>\n",
       "      <td>VERB</td>\n",
       "      <td>prangern</td>\n",
       "      <td>prangeren</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1473</th>\n",
       "      <td>Drewermann sprenge nicht das theologische Trad...</td>\n",
       "      <td>wegführen</td>\n",
       "      <td>VVINF</td>\n",
       "      <td>wegführen</td>\n",
       "      <td>VERB</td>\n",
       "      <td>wegführen</td>\n",
       "      <td>wegfahren</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2116</th>\n",
       "      <td>Wie der iranische Geistliche in einem am Sonnt...</td>\n",
       "      <td>interessiert</td>\n",
       "      <td>ADJD</td>\n",
       "      <td>interessiert</td>\n",
       "      <td>VERB</td>\n",
       "      <td>interessieren</td>\n",
       "      <td>interessieren</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2478</th>\n",
       "      <td>Es ist so aufgestellt daß der Ministerpräsiden...</td>\n",
       "      <td>angekündigt</td>\n",
       "      <td>ADJD</td>\n",
       "      <td>angekündigt</td>\n",
       "      <td>VERB</td>\n",
       "      <td>ankündigen</td>\n",
       "      <td>ankündigen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2561</th>\n",
       "      <td>Das Wort Stasi im Zusammenhang mit Stolpe ist ...</td>\n",
       "      <td>gefallen</td>\n",
       "      <td>VVPP</td>\n",
       "      <td>fallen</td>\n",
       "      <td>VERB</td>\n",
       "      <td>fallen</td>\n",
       "      <td>gefallen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2626</th>\n",
       "      <td>Frieder Otto Wolf von der Eurocom prangerte di...</td>\n",
       "      <td>prangerte</td>\n",
       "      <td>VVFIN</td>\n",
       "      <td>prangern</td>\n",
       "      <td>VERB</td>\n",
       "      <td>prangern</td>\n",
       "      <td>prangeren</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2698</th>\n",
       "      <td>Gäbe es nicht Arbeitsbeschaffungsmaßnahmen ABM...</td>\n",
       "      <td>betrüge</td>\n",
       "      <td>VVFIN</td>\n",
       "      <td>betragen</td>\n",
       "      <td>VERB</td>\n",
       "      <td>betrügen</td>\n",
       "      <td>betrügen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2803</th>\n",
       "      <td>Bush traf unterdessen nicht wie vorgesehen zu ...</td>\n",
       "      <td>vorgesehen</td>\n",
       "      <td>ADJD</td>\n",
       "      <td>vorgesehen</td>\n",
       "      <td>VERB</td>\n",
       "      <td>vorsehen</td>\n",
       "      <td>vorsehen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2854</th>\n",
       "      <td>Die gelegentlich schon erörterte Notwendigkeit...</td>\n",
       "      <td>gehört</td>\n",
       "      <td>VVPP</td>\n",
       "      <td>gehören</td>\n",
       "      <td>VERB</td>\n",
       "      <td>gehören</td>\n",
       "      <td>hören</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2873</th>\n",
       "      <td>umdrahen und verschwinden</td>\n",
       "      <td>umdrahen</td>\n",
       "      <td>VVINF</td>\n",
       "      <td>umdrehen</td>\n",
       "      <td>VERB</td>\n",
       "      <td>umdrehen</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2898</th>\n",
       "      <td>Frieder Otto Wolf von der Eurocom prangerte di...</td>\n",
       "      <td>prangerte</td>\n",
       "      <td>VVFIN</td>\n",
       "      <td>prangern</td>\n",
       "      <td>VERB</td>\n",
       "      <td>prangern</td>\n",
       "      <td>prangeren</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sentence          word  \\\n",
       "98    Den Herren Rao und Singh gebührt ein Platz in ...     resumiert   \n",
       "526   Zur Bedarfsdeckung müßten bis 2000 rund 90 000...   hinzukommen   \n",
       "565   Erst mit Hilfe der aus anderen Orten herbeigee...        gelang   \n",
       "1008  Doch die meisten der nach der Wende rund 4500 ...   verschuldet   \n",
       "1147  Als positiv führen die Experten an daß Versorg...        führen   \n",
       "1202  Aber dann spannt der Gigant die Muskeln die Fe...        spannt   \n",
       "1228  Wie nicht anders zu erwarten rutschten die mei...      dotierte   \n",
       "1278  Von den Banken gelang es während der vergangen...        gelang   \n",
       "1436  Der Münsteraner Theologe Tiemo Rainer Peters p...      prangert   \n",
       "1437  Einen Gleichklang zwischen verstärkten Tendenz...      prangert   \n",
       "1473  Drewermann sprenge nicht das theologische Trad...     wegführen   \n",
       "2116  Wie der iranische Geistliche in einem am Sonnt...  interessiert   \n",
       "2478  Es ist so aufgestellt daß der Ministerpräsiden...   angekündigt   \n",
       "2561  Das Wort Stasi im Zusammenhang mit Stolpe ist ...      gefallen   \n",
       "2626  Frieder Otto Wolf von der Eurocom prangerte di...     prangerte   \n",
       "2698  Gäbe es nicht Arbeitsbeschaffungsmaßnahmen ABM...       betrüge   \n",
       "2803  Bush traf unterdessen nicht wie vorgesehen zu ...    vorgesehen   \n",
       "2854  Die gelegentlich schon erörterte Notwendigkeit...        gehört   \n",
       "2873                          umdrahen und verschwinden      umdrahen   \n",
       "2898  Frieder Otto Wolf von der Eurocom prangerte di...     prangerte   \n",
       "\n",
       "     tiger_pos   tiger_lemma spacy_pos    spacy_lemma     pred_lemma  \n",
       "98       VVFIN    resumieren      VERB     resumieren           None  \n",
       "526      VVINF   hinzukommen      VERB    hinzukommen      hinkommen  \n",
       "565      VVFIN      gelingen      VERB       gelingen       gelangen  \n",
       "1008      ADJD   verschuldet      VERB    verschulden    verschulden  \n",
       "1147     VVFIN        fahren      VERB         fahren         führen  \n",
       "1202     VVFIN       spannen      VERB        spannen        spinnen  \n",
       "1228      ADJA       dotiert      VERB        dotiert       dotieren  \n",
       "1278     VVFIN      gelingen      VERB       gelingen       gelangen  \n",
       "1436     VVFIN      prangern      VERB       prangern      prangeren  \n",
       "1437     VVFIN      prangern      VERB       prangern      prangeren  \n",
       "1473     VVINF     wegführen      VERB      wegführen      wegfahren  \n",
       "2116      ADJD  interessiert      VERB  interessieren  interessieren  \n",
       "2478      ADJD   angekündigt      VERB     ankündigen     ankündigen  \n",
       "2561      VVPP        fallen      VERB         fallen       gefallen  \n",
       "2626     VVFIN      prangern      VERB       prangern      prangeren  \n",
       "2698     VVFIN      betragen      VERB       betrügen       betrügen  \n",
       "2803      ADJD    vorgesehen      VERB       vorsehen       vorsehen  \n",
       "2854      VVPP       gehören      VERB        gehören          hören  \n",
       "2873     VVINF      umdrehen      VERB       umdrehen           None  \n",
       "2898     VVFIN      prangern      VERB       prangern      prangeren  "
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fails_df = tiger_res[tiger_res.tiger_lemma!=tiger_res.pred_lemma]#.drop_duplicates(subset=['word','tiger_lemma'])\n",
    "fails_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "b97e1e70-9dd5-4076-bc9b-b1d35797f9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence='Es ist wichtig, dich zu bewerten'\n",
    "#sentence='Zum neuen Stil gehört daß die Junge Union nicht mehr allein im Saft eigener Vorstellungen schmoren will sondern gezielt andere Parteien zu Wort kommen läßt'\n",
    "\n",
    "doc =spacy_model(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "f3e4b65e-dc93-4f19-90ff-1d0a90b9376a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bewerten re es bewerten\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    if token.pos_=='VERB' and token.text=='bewerten':\n",
    "        lemma = lemmatizer(spacy_token=token)\n",
    "        print(token, token.dep_, token.head.lemma_, lemma)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af64ab0a-edb0-4595-8fc0-4b9d076272c2",
   "metadata": {},
   "source": [
    "## Test HDT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "df8c2f5b-2c6f-434e-bc1b-059593b4ca05",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdt_dataset = 'data/third-party/UD_German-HDT/de_hdt-ud-train-a-1.conllu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "d07aa9f6-a6aa-48d9-9b49-9e2f13e520ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_hdt():\n",
    "    \n",
    "    sentences = []\n",
    "    words = []\n",
    "\n",
    "    prev_sent_id = ''\n",
    "    \n",
    "    with open(hdt_dataset,'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            if 'sent_id' in line:\n",
    "                sent_id = re.search(r'sent_id = (.+)',line).groups(1)\n",
    "                if sent_id!=prev_sent_id:\n",
    "                    if words:\n",
    "                        sentences.append(words)\n",
    "                    words = []\n",
    "                    prev_sent_id = sent_id\n",
    "            else:\n",
    "                s = re.match(r'[0-9]+\\t([\\w]+)\\t([\\w]+)\\t([\\w]+)',line)\n",
    "                if s:\n",
    "                    words.append(s.groups(0))\n",
    "\n",
    "    return sentences\n",
    "\n",
    "sentences_hdt = read_hdt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "babac098-4bb7-403b-a6b3-411f9f712cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1999\n"
     ]
    }
   ],
   "source": [
    "hdt_res = []\n",
    "\n",
    "for idx,sentence in enumerate(sentences_hdt):\n",
    "    words, lemmas, pos = zip(*sentence)\n",
    "    text = ' '.join(words)\n",
    "    doc = spacy_model(text)\n",
    "    if len(doc)==len(lemmas):\n",
    "        for token, hdt_word, hdt_lemma, hdt_pos in zip(doc,words,lemmas,pos):\n",
    "             if hdt_word == token.text:\n",
    "                if token.pos_=='VERB':\n",
    "                   lemma = lemmatizer(spacy_token=token)\n",
    "                   hdt_res.append((text,hdt_word, hdt_pos, hdt_lemma, token.pos_, token.lemma_, lemma))\n",
    "    if (idx+1)%2000==0:\n",
    "        print(idx)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "b226d0fd-30de-4a7b-91e6-03616da25030",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdt_res = pd.DataFrame(hdt_res, columns = ['sentence','word','hdt_pos','hdt_lemma','spacy_pos','spacy_lemma','pred_lemma'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "97631068-de8d-4e4d-b10a-42749e8eb5d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9800214056368177"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((hdt_res.hdt_pos.str.startswith('V')) & (hdt_res.spacy_pos=='VERB')).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "27c6a086-9aa4-4599-8ae4-63dceab5decf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9921512665001784"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(hdt_res.hdt_lemma==hdt_res.pred_lemma).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "690fda5a-d20a-49e7-b594-219936847bb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9700321084552266"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(hdt_res.hdt_lemma==hdt_res.spacy_lemma).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "6b609a38-1f34-4577-93a1-dfff7111be72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>word</th>\n",
       "      <th>hdt_pos</th>\n",
       "      <th>hdt_lemma</th>\n",
       "      <th>spacy_pos</th>\n",
       "      <th>spacy_lemma</th>\n",
       "      <th>pred_lemma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>583</th>\n",
       "      <td>Und das Angebot der Telekom dafür Leitungen vo...</td>\n",
       "      <td>führe</td>\n",
       "      <td>VERB</td>\n",
       "      <td>fahren</td>\n",
       "      <td>VERB</td>\n",
       "      <td>führen</td>\n",
       "      <td>führen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>892</th>\n",
       "      <td>HP will nach gutem Geschäftsergebnis Aktien sp...</td>\n",
       "      <td>splitten</td>\n",
       "      <td>VERB</td>\n",
       "      <td>splitten</td>\n",
       "      <td>VERB</td>\n",
       "      <td>splitten</td>\n",
       "      <td>spleiden</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1110</th>\n",
       "      <td>Auch bei dem Streit um die Rechtmäßigkeit der ...</td>\n",
       "      <td>gelang</td>\n",
       "      <td>VERB</td>\n",
       "      <td>gelingen</td>\n",
       "      <td>VERB</td>\n",
       "      <td>gelingen</td>\n",
       "      <td>gelangen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1236</th>\n",
       "      <td>Die Aktien sollen in dem Verhältnis eins zu zw...</td>\n",
       "      <td>gesplittet</td>\n",
       "      <td>VERB</td>\n",
       "      <td>splitten</td>\n",
       "      <td>VERB</td>\n",
       "      <td>splitten</td>\n",
       "      <td>gespleiden</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1241</th>\n",
       "      <td>Bereits in der Vergangeheit wurden die Aktien ...</td>\n",
       "      <td>gesplittet</td>\n",
       "      <td>VERB</td>\n",
       "      <td>splitten</td>\n",
       "      <td>VERB</td>\n",
       "      <td>splitten</td>\n",
       "      <td>gespleiden</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1787</th>\n",
       "      <td>Über verschiedene auf diesem Portal soll das G...</td>\n",
       "      <td>hereinkmmen</td>\n",
       "      <td>VERB</td>\n",
       "      <td>unknown</td>\n",
       "      <td>VERB</td>\n",
       "      <td>hereinkmmen</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2052</th>\n",
       "      <td>Der Bereich EADS Telecommunications unter sein...</td>\n",
       "      <td>gehört</td>\n",
       "      <td>VERB</td>\n",
       "      <td>gehören</td>\n",
       "      <td>VERB</td>\n",
       "      <td>gehören</td>\n",
       "      <td>hören</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2465</th>\n",
       "      <td>Einem Bericht der Saarbrücker Zeitung zufolge ...</td>\n",
       "      <td>zusamenarbeiten</td>\n",
       "      <td>VERB</td>\n",
       "      <td>unknown</td>\n",
       "      <td>VERB</td>\n",
       "      <td>zusamenarbein</td>\n",
       "      <td>zusamenarbeiten</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2628</th>\n",
       "      <td>Durchschnittlich fast fünfeinhalb Millionen in...</td>\n",
       "      <td>gebracht</td>\n",
       "      <td>VERB</td>\n",
       "      <td>bringen</td>\n",
       "      <td>VERB</td>\n",
       "      <td>bringen</td>\n",
       "      <td>gebrechen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2770</th>\n",
       "      <td>Durch die neuen Netzwerke entwickele sich die ...</td>\n",
       "      <td>entwickele</td>\n",
       "      <td>VERB</td>\n",
       "      <td>entwickeln</td>\n",
       "      <td>VERB</td>\n",
       "      <td>entwickeln</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sentence             word  \\\n",
       "583   Und das Angebot der Telekom dafür Leitungen vo...            führe   \n",
       "892   HP will nach gutem Geschäftsergebnis Aktien sp...         splitten   \n",
       "1110  Auch bei dem Streit um die Rechtmäßigkeit der ...           gelang   \n",
       "1236  Die Aktien sollen in dem Verhältnis eins zu zw...       gesplittet   \n",
       "1241  Bereits in der Vergangeheit wurden die Aktien ...       gesplittet   \n",
       "1787  Über verschiedene auf diesem Portal soll das G...      hereinkmmen   \n",
       "2052  Der Bereich EADS Telecommunications unter sein...           gehört   \n",
       "2465  Einem Bericht der Saarbrücker Zeitung zufolge ...  zusamenarbeiten   \n",
       "2628  Durchschnittlich fast fünfeinhalb Millionen in...         gebracht   \n",
       "2770  Durch die neuen Netzwerke entwickele sich die ...       entwickele   \n",
       "\n",
       "     hdt_pos   hdt_lemma spacy_pos    spacy_lemma       pred_lemma  \n",
       "583     VERB      fahren      VERB         führen           führen  \n",
       "892     VERB    splitten      VERB       splitten         spleiden  \n",
       "1110    VERB    gelingen      VERB       gelingen         gelangen  \n",
       "1236    VERB    splitten      VERB       splitten       gespleiden  \n",
       "1241    VERB    splitten      VERB       splitten       gespleiden  \n",
       "1787    VERB     unknown      VERB    hereinkmmen             None  \n",
       "2052    VERB     gehören      VERB        gehören            hören  \n",
       "2465    VERB     unknown      VERB  zusamenarbein  zusamenarbeiten  \n",
       "2628    VERB     bringen      VERB        bringen        gebrechen  \n",
       "2770    VERB  entwickeln      VERB     entwickeln             None  "
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fails_df = hdt_res[hdt_res.hdt_lemma!=hdt_res.pred_lemma]#.drop_duplicates(subset=['word','tiger_lemma'])\n",
    "fails_df[fails_df.hdt_pos=='VERB']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "703b18f0-f32d-4923-addd-c20a4c052c34",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'entwickele'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[270], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mlemmatizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mentwickele\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'entwickele'"
     ]
    }
   ],
   "source": [
    "lemmatizer.verbs['entwickele']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "16e87cf5-fd5b-4c5e-b32f-1e79c3802e96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Der Bereich EADS Telecommunications unter seiner Leitung wird bei der EADS Defence Security Networks angesiedelt zu der auch die AEG Mobile Communications Ulm gehört'"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fails_df.loc[2052].sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "10e5bb0f-c297-4c6a-b24e-f0e6a4539c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence='Es ist wichtig, dich zu bewerten'\n",
    "sentence='Der Bereich EADS Telecommunications unter seiner Leitung wird bei der EADS Defence Security Networks angesiedelt zu der auch die AEG Mobile Communications Ulm gehört'\n",
    "\n",
    "doc =spacy_model(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "7b4f9f3a-9541-4b1c-9018-524f185ad2bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gehört cj werden hören\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    if token.pos_=='VERB' and token.text=='gehört':\n",
    "        lemma = lemmatizer(spacy_token=token)\n",
    "        print(token, token.dep_, token.head.lemma_, lemma)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd01975c-5943-4eb3-ad0e-9dc9893e063c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
